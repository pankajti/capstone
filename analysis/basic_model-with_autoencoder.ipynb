{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import *\n",
    "from tensorflow import keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_digits\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from tensorboard.plugins import projector\n",
    "\n",
    "root_dir = r'/Users/pankaj/Library/Mobile Documents/com~apple~CloudDocs/Capstone/Wikipedia Data'\n",
    "\n",
    "\n",
    "result_path = annot_file_path = os.path.join(root_dir , 'comments_with_grouped_annoptations.tsv')\n",
    "\n",
    "merged_comments = pd.read_table(result_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_comments['recipient_attack'] = merged_comments['recipient_attack'].apply(lambda x : 1 if x> 1 else 0 )\n",
    "X_train = merged_comments['comment']\n",
    "y = merged_comments['recipient_attack']\n",
    "one_hot_train_labels = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_comments['new_attack'] = merged_comments['attack'].apply(lambda x : 1 if x> 1 else 0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO change model with proper word settings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 187899 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# Prepare training input  \n",
    "training_samples = 90000\n",
    "validation_samples = 10000\n",
    "max_words = 15000\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences = tokenizer.texts_to_sequences(X_train)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (115864, 150)\n"
     ]
    }
   ],
   "source": [
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "x_train = data[:training_samples]\n",
    "x_val = data[training_samples: (training_samples + validation_samples)]\n",
    "x_test = data[(training_samples + validation_samples) : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (115864, 2)\n"
     ]
    }
   ],
   "source": [
    "# Prepare labels \n",
    "labels = np.asarray(one_hot_train_labels)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "labels = labels[indices]\n",
    "y_train = labels[:training_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]\n",
    "y_test = labels[(training_samples + validation_samples) : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 16)          240000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                6272      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 247,394\n",
      "Trainable params: 247,394\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM\n",
    "embedding_dim = 16\n",
    "model = Sequential()\n",
    "#embedding = Embedding(max_words, embedding_dim , weights =w)\n",
    "embedding = Embedding(max_words, embedding_dim)\n",
    "\n",
    "#embedding.trainable = False\n",
    "model.add(embedding)\n",
    "model.add(LSTM(32))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "90000/90000 [==============================] - 81s 902us/sample - loss: 0.3725 - acc: 0.8441 - val_loss: 0.3231 - val_acc: 0.8702\n",
      "Epoch 2/2\n",
      "90000/90000 [==============================] - 80s 893us/sample - loss: 0.3013 - acc: 0.8768 - val_loss: 0.3185 - val_acc: 0.8707\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "loss='binary_crossentropy',\n",
    "metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,\n",
    "epochs= 2,\n",
    "batch_size=128,\n",
    "validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('basic_lstm_with_auto_encoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index_1 = {k, v+3 for k, v in word_index.items()}\n",
    "word_index_1['<padding>'] = 0\n",
    "word_index_1['<start>'] = 1\n",
    "word_index_1['<unknown>'] = 2\n",
    "\n",
    "counter = 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'newline': 2,\n",
       " 'to': 3,\n",
       " 'tokennewline': 4,\n",
       " 'and': 5,\n",
       " 'of': 6,\n",
       " 'a': 7,\n",
       " 'you': 8,\n",
       " 'token': 9,\n",
       " 'i': 10,\n",
       " 'is': 11,\n",
       " 'that': 12,\n",
       " 'in': 13,\n",
       " 'it': 14,\n",
       " 'for': 15,\n",
       " 'not': 16,\n",
       " 'this': 17,\n",
       " 'on': 18,\n",
       " 'be': 19,\n",
       " 'are': 20,\n",
       " 'as': 21,\n",
       " 'have': 22,\n",
       " 'your': 23,\n",
       " 'with': 24,\n",
       " 'was': 25,\n",
       " 'or': 26,\n",
       " 'article': 27,\n",
       " 'if': 28,\n",
       " 'but': 29,\n",
       " 'my': 30,\n",
       " 'an': 31,\n",
       " 'by': 32,\n",
       " 'wikipedia': 33,\n",
       " 'page': 34,\n",
       " 'from': 35,\n",
       " 'do': 36,\n",
       " 'me': 37,\n",
       " 'about': 38,\n",
       " 'at': 39,\n",
       " 'so': 40,\n",
       " 'all': 41,\n",
       " 'what': 42,\n",
       " 'can': 43,\n",
       " 'will': 44,\n",
       " 'has': 45,\n",
       " 'no': 46,\n",
       " 'there': 47,\n",
       " 'he': 48,\n",
       " 'like': 49,\n",
       " 'they': 50,\n",
       " 'one': 51,\n",
       " 'would': 52,\n",
       " 'just': 53,\n",
       " 'which': 54,\n",
       " 'who': 55,\n",
       " 'been': 56,\n",
       " 'any': 57,\n",
       " \"don't\": 58,\n",
       " 'should': 59,\n",
       " 'his': 60,\n",
       " 'we': 61,\n",
       " 'more': 62,\n",
       " 'talk': 63,\n",
       " 'some': 64,\n",
       " 'other': 65,\n",
       " 'please': 66,\n",
       " 'am': 67,\n",
       " 'here': 68,\n",
       " 'think': 69,\n",
       " 'know': 70,\n",
       " 'because': 71,\n",
       " 'see': 72,\n",
       " 'people': 73,\n",
       " 'also': 74,\n",
       " 'up': 75,\n",
       " 'fuck': 76,\n",
       " 'only': 77,\n",
       " 'how': 78,\n",
       " 'tokeni': 79,\n",
       " 'out': 80,\n",
       " \"it's\": 81,\n",
       " 'why': 82,\n",
       " 'when': 83,\n",
       " 'edit': 84,\n",
       " 'were': 85,\n",
       " 'then': 86,\n",
       " \"i'm\": 87,\n",
       " 'time': 88,\n",
       " 'their': 89,\n",
       " 'them': 90,\n",
       " 'being': 91,\n",
       " 'did': 92,\n",
       " 'now': 93,\n",
       " 'articles': 94,\n",
       " 'even': 95,\n",
       " 'than': 96,\n",
       " 'get': 97,\n",
       " 'use': 98,\n",
       " 'had': 99,\n",
       " 'may': 100,\n",
       " 'make': 101,\n",
       " 'very': 102,\n",
       " 'good': 103,\n",
       " 'does': 104,\n",
       " 'well': 105,\n",
       " 'go': 106,\n",
       " 'want': 107,\n",
       " 'way': 108,\n",
       " 'information': 109,\n",
       " 'such': 110,\n",
       " 'its': 111,\n",
       " 'could': 112,\n",
       " 'name': 113,\n",
       " 'user': 114,\n",
       " 'these': 115,\n",
       " 'say': 116,\n",
       " 'sources': 117,\n",
       " 'first': 118,\n",
       " 'made': 119,\n",
       " 'again': 120,\n",
       " 'edits': 121,\n",
       " 'section': 122,\n",
       " 'source': 123,\n",
       " 'new': 124,\n",
       " 'many': 125,\n",
       " 'wp': 126,\n",
       " 'need': 127,\n",
       " 'where': 128,\n",
       " 'over': 129,\n",
       " 'much': 130,\n",
       " 'wiki': 131,\n",
       " 'really': 132,\n",
       " 'same': 133,\n",
       " 'right': 134,\n",
       " 'most': 135,\n",
       " 'him': 136,\n",
       " 'editing': 137,\n",
       " 'those': 138,\n",
       " 'into': 139,\n",
       " 'thanks': 140,\n",
       " 'used': 141,\n",
       " 'point': 142,\n",
       " 'since': 143,\n",
       " 'help': 144,\n",
       " 'before': 145,\n",
       " 'going': 146,\n",
       " 'pages': 147,\n",
       " 'read': 148,\n",
       " 'said': 149,\n",
       " 'find': 150,\n",
       " 'work': 151,\n",
       " 'discussion': 152,\n",
       " 'still': 153,\n",
       " 'fact': 154,\n",
       " 'after': 155,\n",
       " 'someone': 156,\n",
       " 'own': 157,\n",
       " 'back': 158,\n",
       " 'two': 159,\n",
       " 'something': 160,\n",
       " 'her': 161,\n",
       " 'take': 162,\n",
       " 'nigger': 163,\n",
       " 'image': 164,\n",
       " 'too': 165,\n",
       " 'look': 166,\n",
       " 'stop': 167,\n",
       " 'deletion': 168,\n",
       " 'block': 169,\n",
       " '2': 170,\n",
       " 'style': 171,\n",
       " 'tokenthe': 172,\n",
       " 'u': 173,\n",
       " 'blocked': 174,\n",
       " \"i've\": 175,\n",
       " 'shit': 176,\n",
       " 'deleted': 177,\n",
       " 'faggot': 178,\n",
       " 'without': 179,\n",
       " 'history': 180,\n",
       " \"you're\": 181,\n",
       " 'add': 182,\n",
       " 'yourself': 183,\n",
       " 'never': 184,\n",
       " 'off': 185,\n",
       " 'list': 186,\n",
       " 'personal': 187,\n",
       " 'bla': 188,\n",
       " 'suck': 189,\n",
       " '1': 190,\n",
       " 'editors': 191,\n",
       " 'thank': 192,\n",
       " 'vandalism': 193,\n",
       " 'added': 194,\n",
       " 'another': 195,\n",
       " 'link': 196,\n",
       " 'she': 197,\n",
       " 'us': 198,\n",
       " 'reason': 199,\n",
       " 'against': 200,\n",
       " 'put': 201,\n",
       " \"doesn't\": 202,\n",
       " 'anything': 203,\n",
       " 'removed': 204,\n",
       " 'case': 205,\n",
       " 'person': 206,\n",
       " 'hate': 207,\n",
       " 'content': 208,\n",
       " 'better': 209,\n",
       " 'our': 210,\n",
       " 'ass': 211,\n",
       " 'comment': 212,\n",
       " 'actually': 213,\n",
       " 'things': 214,\n",
       " 'seems': 215,\n",
       " 'sure': 216,\n",
       " 'place': 217,\n",
       " \"that's\": 218,\n",
       " 'under': 219,\n",
       " 'might': 220,\n",
       " \"can't\": 221,\n",
       " 'nothing': 222,\n",
       " 'done': 223,\n",
       " 'however': 224,\n",
       " 'both': 225,\n",
       " 'tokenyou': 226,\n",
       " 'while': 227,\n",
       " 'believe': 228,\n",
       " 'part': 229,\n",
       " 'wrong': 230,\n",
       " 'thing': 231,\n",
       " 'using': 232,\n",
       " 'keep': 233,\n",
       " 'comments': 234,\n",
       " 'feel': 235,\n",
       " 'com': 236,\n",
       " \"didn't\": 237,\n",
       " 'hi': 238,\n",
       " 'free': 239,\n",
       " 'trying': 240,\n",
       " 'little': 241,\n",
       " 'links': 242,\n",
       " 'must': 243,\n",
       " 'question': 244,\n",
       " 'world': 245,\n",
       " 'anyone': 246,\n",
       " 'understand': 247,\n",
       " 'already': 248,\n",
       " 'others': 249,\n",
       " 'policy': 250,\n",
       " 'ask': 251,\n",
       " 'http': 252,\n",
       " 'change': 253,\n",
       " 'note': 254,\n",
       " 'long': 255,\n",
       " 'though': 256,\n",
       " 'problem': 257,\n",
       " 'give': 258,\n",
       " 'above': 259,\n",
       " 'years': 260,\n",
       " '•': 261,\n",
       " 'editor': 262,\n",
       " 'remove': 263,\n",
       " 'come': 264,\n",
       " 'making': 265,\n",
       " 'fucking': 266,\n",
       " 'issue': 267,\n",
       " 'different': 268,\n",
       " 'english': 269,\n",
       " 'last': 270,\n",
       " 'fat': 271,\n",
       " 'life': 272,\n",
       " 'agree': 273,\n",
       " 'few': 274,\n",
       " 'hope': 275,\n",
       " 'gay': 276,\n",
       " 'admin': 277,\n",
       " \"i'll\": 278,\n",
       " 'mean': 279,\n",
       " 'word': 280,\n",
       " '3': 281,\n",
       " 'doing': 282,\n",
       " 'rather': 283,\n",
       " 'every': 284,\n",
       " 'reliable': 285,\n",
       " 'reference': 286,\n",
       " 'best': 287,\n",
       " 'state': 288,\n",
       " 'post': 289,\n",
       " 'called': 290,\n",
       " 'try': 291,\n",
       " 'simply': 292,\n",
       " 'real': 293,\n",
       " 'got': 294,\n",
       " 'down': 295,\n",
       " 'let': 296,\n",
       " 'says': 297,\n",
       " 'subject': 298,\n",
       " 'oh': 299,\n",
       " 'day': 300,\n",
       " 'penis': 301,\n",
       " 'top': 302,\n",
       " 'non': 303,\n",
       " 'else': 304,\n",
       " 'site': 305,\n",
       " 'tokenthis': 306,\n",
       " 'delete': 307,\n",
       " 'die': 308,\n",
       " 'war': 309,\n",
       " 'opinion': 310,\n",
       " 'yet': 311,\n",
       " 'show': 312,\n",
       " 'least': 313,\n",
       " 'www': 314,\n",
       " 'sorry': 315,\n",
       " 'tell': 316,\n",
       " 'text': 317,\n",
       " 'welcome': 318,\n",
       " 'ip': 319,\n",
       " 'either': 320,\n",
       " 'great': 321,\n",
       " \"isn't\": 322,\n",
       " 'between': 323,\n",
       " 'found': 324,\n",
       " 'users': 325,\n",
       " 'ever': 326,\n",
       " 'probably': 327,\n",
       " 'etc': 328,\n",
       " 'enough': 329,\n",
       " 'matter': 330,\n",
       " 'book': 331,\n",
       " 'far': 332,\n",
       " 'evidence': 333,\n",
       " 'references': 334,\n",
       " 'view': 335,\n",
       " 'clearly': 336,\n",
       " 'e': 337,\n",
       " 'utc': 338,\n",
       " 'through': 339,\n",
       " 'leave': 340,\n",
       " 'org': 341,\n",
       " 'example': 342,\n",
       " 'color': 343,\n",
       " 'saying': 344,\n",
       " 'language': 345,\n",
       " 'original': 346,\n",
       " 'around': 347,\n",
       " 'consensus': 348,\n",
       " 'thought': 349,\n",
       " 'true': 350,\n",
       " 'jew': 351,\n",
       " 'old': 352,\n",
       " 'bad': 353,\n",
       " 'pov': 354,\n",
       " 'cunt': 355,\n",
       " 'account': 356,\n",
       " 'revert': 357,\n",
       " 'clear': 358,\n",
       " 'given': 359,\n",
       " 'needs': 360,\n",
       " 'support': 361,\n",
       " 'continue': 362,\n",
       " 'request': 363,\n",
       " 'having': 364,\n",
       " 's': 365,\n",
       " 'claim': 366,\n",
       " 'written': 367,\n",
       " 'questions': 368,\n",
       " 'love': 369,\n",
       " 'instead': 370,\n",
       " 'times': 371,\n",
       " 'until': 372,\n",
       " 'term': 373,\n",
       " 'always': 374,\n",
       " 'number': 375,\n",
       " 'maybe': 376,\n",
       " 'lot': 377,\n",
       " 'write': 378,\n",
       " 'seem': 379,\n",
       " 'material': 380,\n",
       " 'important': 381,\n",
       " 'correct': 382,\n",
       " 'states': 383,\n",
       " 'facts': 384,\n",
       " 'title': 385,\n",
       " 'dont': 386,\n",
       " 'based': 387,\n",
       " 'yes': 388,\n",
       " 'reverted': 389,\n",
       " 'tag': 390,\n",
       " 'attack': 391,\n",
       " 'care': 392,\n",
       " 'fair': 393,\n",
       " 'words': 394,\n",
       " 'further': 395,\n",
       " 'makes': 396,\n",
       " 'mention': 397,\n",
       " '0': 398,\n",
       " 'dick': 399,\n",
       " 'check': 400,\n",
       " '5': 401,\n",
       " 'perhaps': 402,\n",
       " 'research': 403,\n",
       " 'rules': 404,\n",
       " 'background': 405,\n",
       " 'quite': 406,\n",
       " 'left': 407,\n",
       " 'message': 408,\n",
       " 'align': 409,\n",
       " 'notable': 410,\n",
       " 'created': 411,\n",
       " 'american': 412,\n",
       " 'hey': 413,\n",
       " 'man': 414,\n",
       " 'adding': 415,\n",
       " 'bit': 416,\n",
       " 'cannot': 417,\n",
       " 'three': 418,\n",
       " 'whether': 419,\n",
       " 'idea': 420,\n",
       " 're': 421,\n",
       " 'version': 422,\n",
       " '4': 423,\n",
       " 'once': 424,\n",
       " 'tokenplease': 425,\n",
       " 'means': 426,\n",
       " 'tokenhi': 427,\n",
       " 'year': 428,\n",
       " 'big': 429,\n",
       " 'copyright': 430,\n",
       " 'several': 431,\n",
       " 'each': 432,\n",
       " \"i'd\": 433,\n",
       " 'getting': 434,\n",
       " 'website': 435,\n",
       " 'tokenif': 436,\n",
       " 'en': 437,\n",
       " 'call': 438,\n",
       " 'encyclopedia': 439,\n",
       " 'known': 440,\n",
       " 'whole': 441,\n",
       " 'considered': 442,\n",
       " 'consider': 443,\n",
       " 'group': 444,\n",
       " 'bitch': 445,\n",
       " 'kind': 446,\n",
       " 'current': 447,\n",
       " 'course': 448,\n",
       " 'second': 449,\n",
       " 'mentioned': 450,\n",
       " 'following': 451,\n",
       " '—': 452,\n",
       " 'changes': 453,\n",
       " 'statement': 454,\n",
       " 'd': 455,\n",
       " 'live': 456,\n",
       " 'images': 457,\n",
       " 'review': 458,\n",
       " 'seen': 459,\n",
       " 'less': 460,\n",
       " 'moron': 461,\n",
       " 'topic': 462,\n",
       " 'speedy': 463,\n",
       " \"there's\": 464,\n",
       " 'media': 465,\n",
       " 'sense': 466,\n",
       " 'start': 467,\n",
       " 'possible': 468,\n",
       " 'issues': 469,\n",
       " 'including': 470,\n",
       " 'end': 471,\n",
       " 'claims': 472,\n",
       " '000': 473,\n",
       " 'away': 474,\n",
       " 'include': 475,\n",
       " 'removing': 476,\n",
       " 'main': 477,\n",
       " 'general': 478,\n",
       " 'attacks': 479,\n",
       " 'desu': 480,\n",
       " 'address': 481,\n",
       " 'days': 482,\n",
       " 'myself': 483,\n",
       " 'info': 484,\n",
       " 'related': 485,\n",
       " 'suggest': 486,\n",
       " 'tokenhttp': 487,\n",
       " 'contributions': 488,\n",
       " 'template': 489,\n",
       " 'neutral': 490,\n",
       " 'truth': 491,\n",
       " 'community': 492,\n",
       " 'stupid': 493,\n",
       " 'explain': 494,\n",
       " 'regarding': 495,\n",
       " 'listed': 496,\n",
       " 'line': 497,\n",
       " 'warning': 498,\n",
       " 'news': 499,\n",
       " 'wrote': 500,\n",
       " 'public': 501,\n",
       " 'anti': 502,\n",
       " 'anyway': 503,\n",
       " 'school': 504,\n",
       " '10': 505,\n",
       " 'faith': 506,\n",
       " 'changed': 507,\n",
       " 'mind': 508,\n",
       " 'provide': 509,\n",
       " 'piece': 510,\n",
       " 'order': 511,\n",
       " 'everyone': 512,\n",
       " 'notice': 513,\n",
       " 'guys': 514,\n",
       " 'taken': 515,\n",
       " 'self': 516,\n",
       " 'later': 517,\n",
       " 'god': 518,\n",
       " 'false': 519,\n",
       " 'date': 520,\n",
       " 'next': 521,\n",
       " 'official': 522,\n",
       " 'vertical': 523,\n",
       " 'sentence': 524,\n",
       " 'jews': 525,\n",
       " 'completely': 526,\n",
       " 'report': 527,\n",
       " 'admins': 528,\n",
       " 'picture': 529,\n",
       " 'redirect': 530,\n",
       " 'single': 531,\n",
       " '100': 532,\n",
       " 'relevant': 533,\n",
       " 'full': 534,\n",
       " 'within': 535,\n",
       " 'talking': 536,\n",
       " 'stuff': 537,\n",
       " 'country': 538,\n",
       " 'dickhead': 539,\n",
       " 'nice': 540,\n",
       " 'sucks': 541,\n",
       " 'power': 542,\n",
       " 'common': 543,\n",
       " 'included': 544,\n",
       " 'wanker': 545,\n",
       " 'due': 546,\n",
       " 'especially': 547,\n",
       " 'political': 548,\n",
       " 'looking': 549,\n",
       " 'interest': 550,\n",
       " 'itself': 551,\n",
       " 'guidelines': 552,\n",
       " 'freedom': 553,\n",
       " 'answer': 554,\n",
       " 'wanted': 555,\n",
       " 'party': 556,\n",
       " 'everything': 557,\n",
       " 'per': 558,\n",
       " 'discuss': 559,\n",
       " 'move': 560,\n",
       " 'banned': 561,\n",
       " 'border': 562,\n",
       " 'project': 563,\n",
       " 'appropriate': 564,\n",
       " 'came': 565,\n",
       " 'during': 566,\n",
       " 'obviously': 567,\n",
       " 'middle': 568,\n",
       " \"he's\": 569,\n",
       " 'recent': 570,\n",
       " 'writing': 571,\n",
       " \"won't\": 572,\n",
       " 'names': 573,\n",
       " 'reading': 574,\n",
       " \"you've\": 575,\n",
       " 'although': 576,\n",
       " 'create': 577,\n",
       " 'british': 578,\n",
       " 'fag': 579,\n",
       " 'involved': 580,\n",
       " 'aids': 581,\n",
       " 'lead': 582,\n",
       " 'able': 583,\n",
       " 'nor': 584,\n",
       " 'certainly': 585,\n",
       " 'books': 586,\n",
       " 'pretty': 587,\n",
       " 'tokenthanks': 588,\n",
       " 'wish': 589,\n",
       " 'whatever': 590,\n",
       " 'hard': 591,\n",
       " 'posted': 592,\n",
       " 'high': 593,\n",
       " 'c': 594,\n",
       " 'ok': 595,\n",
       " 'government': 596,\n",
       " 'started': 597,\n",
       " 'guy': 598,\n",
       " 'according': 599,\n",
       " 'ban': 600,\n",
       " \"wasn't\": 601,\n",
       " 'therefore': 602,\n",
       " 'happy': 603,\n",
       " 'deleting': 604,\n",
       " 'specific': 605,\n",
       " 'response': 606,\n",
       " 'nipple': 607,\n",
       " 'tokenit': 608,\n",
       " 'google': 609,\n",
       " 'nigga': 610,\n",
       " 'exactly': 611,\n",
       " 'past': 612,\n",
       " 'criteria': 613,\n",
       " 'published': 614,\n",
       " 'unless': 615,\n",
       " 'argument': 616,\n",
       " \"wikipedia's\": 617,\n",
       " 'asked': 618,\n",
       " 'four': 619,\n",
       " 'administrator': 620,\n",
       " 'small': 621,\n",
       " 'huge': 622,\n",
       " 'reasons': 623,\n",
       " 'currently': 624,\n",
       " 'game': 625,\n",
       " 'pro': 626,\n",
       " 'cock': 627,\n",
       " 'side': 628,\n",
       " 'ago': 629,\n",
       " 'policies': 630,\n",
       " 'white': 631,\n",
       " 'reverting': 632,\n",
       " 'system': 633,\n",
       " 'rule': 634,\n",
       " 'position': 635,\n",
       " 'p': 636,\n",
       " 'united': 637,\n",
       " 'sign': 638,\n",
       " 'large': 639,\n",
       " 'form': 640,\n",
       " 'npov': 641,\n",
       " 'dispute': 642,\n",
       " 'edited': 643,\n",
       " 'tokenhello': 644,\n",
       " 'interested': 645,\n",
       " 'b': 646,\n",
       " 'tokenin': 647,\n",
       " 'alone': 648,\n",
       " 'appears': 649,\n",
       " 'today': 650,\n",
       " 'tried': 651,\n",
       " 'major': 652,\n",
       " 'looks': 653,\n",
       " 'future': 654,\n",
       " 'category': 655,\n",
       " 'knowledge': 656,\n",
       " 'guess': 657,\n",
       " 'national': 658,\n",
       " 'quote': 659,\n",
       " 'stated': 660,\n",
       " 'summary': 661,\n",
       " 'access': 662,\n",
       " 'law': 663,\n",
       " 'padding': 664,\n",
       " 'often': 665,\n",
       " 'kill': 666,\n",
       " '6': 667,\n",
       " 'width': 668,\n",
       " 'remember': 669,\n",
       " 'along': 670,\n",
       " 'similar': 671,\n",
       " 'paragraph': 672,\n",
       " 'black': 673,\n",
       " 'city': 674,\n",
       " 'film': 675,\n",
       " 'took': 676,\n",
       " 'tokenand': 677,\n",
       " '2006': 678,\n",
       " 'told': 679,\n",
       " 'entire': 680,\n",
       " 'solid': 681,\n",
       " 'entry': 682,\n",
       " 'notability': 683,\n",
       " 'become': 684,\n",
       " 'provided': 685,\n",
       " 'stay': 686,\n",
       " 'class': 687,\n",
       " 'shows': 688,\n",
       " 'needed': 689,\n",
       " 'family': 690,\n",
       " 'bias': 691,\n",
       " 'certain': 692,\n",
       " 'present': 693,\n",
       " 'themselves': 694,\n",
       " 'working': 695,\n",
       " 'almost': 696,\n",
       " '8': 697,\n",
       " 'mr': 698,\n",
       " 'views': 699,\n",
       " 'otherwise': 700,\n",
       " 'taking': 701,\n",
       " 'actions': 702,\n",
       " 'process': 703,\n",
       " 'particular': 704,\n",
       " 'learn': 705,\n",
       " 'tokenwhy': 706,\n",
       " 'internet': 707,\n",
       " 'unblock': 708,\n",
       " 'open': 709,\n",
       " '20': 710,\n",
       " 'sort': 711,\n",
       " '7': 712,\n",
       " 'cited': 713,\n",
       " 'respect': 714,\n",
       " 'faggots': 715,\n",
       " 'points': 716,\n",
       " 'file': 717,\n",
       " 'story': 718,\n",
       " 'problems': 719,\n",
       " 'web': 720,\n",
       " 'allowed': 721,\n",
       " 'theory': 722,\n",
       " 'obvious': 723,\n",
       " 'useful': 724,\n",
       " 'jpg': 725,\n",
       " 'search': 726,\n",
       " 'actual': 727,\n",
       " 'super': 728,\n",
       " 'hell': 729,\n",
       " 'human': 730,\n",
       " 'asshole': 731,\n",
       " 'definition': 732,\n",
       " 'biased': 733,\n",
       " 'terms': 734,\n",
       " 'death': 735,\n",
       " '–': 736,\n",
       " 'simple': 737,\n",
       " 'noes': 738,\n",
       " 'short': 739,\n",
       " \"haven't\": 740,\n",
       " 'appear': 741,\n",
       " 'follow': 742,\n",
       " 'yourselfgo': 743,\n",
       " 'freezer': 744,\n",
       " 'himself': 745,\n",
       " 'smells': 746,\n",
       " '2007': 747,\n",
       " 'below': 748,\n",
       " 'fine': 749,\n",
       " 'g': 750,\n",
       " 'proof': 751,\n",
       " 'noticed': 752,\n",
       " 'indeed': 753,\n",
       " 'im': 754,\n",
       " 'tokenwhat': 755,\n",
       " 'music': 756,\n",
       " 'f': 757,\n",
       " 'hand': 758,\n",
       " 'appreciate': 759,\n",
       " 'john': 760,\n",
       " 'thus': 761,\n",
       " 'act': 762,\n",
       " 'university': 763,\n",
       " 'calling': 764,\n",
       " 'reply': 765,\n",
       " 'sourced': 766,\n",
       " 'likely': 767,\n",
       " 'jewish': 768,\n",
       " 'cite': 769,\n",
       " 'various': 770,\n",
       " 'set': 771,\n",
       " 'aware': 772,\n",
       " 'german': 773,\n",
       " '2008': 774,\n",
       " 'area': 775,\n",
       " 'hours': 776,\n",
       " 'uk': 777,\n",
       " 'result': 778,\n",
       " 'deal': 779,\n",
       " 'longer': 780,\n",
       " 'five': 781,\n",
       " 'status': 782,\n",
       " 'saw': 783,\n",
       " 'citation': 784,\n",
       " 'comes': 785,\n",
       " 'company': 786,\n",
       " 'happened': 787,\n",
       " 'valid': 788,\n",
       " 'damn': 789,\n",
       " 'greek': 790,\n",
       " 'members': 791,\n",
       " \"aren't\": 792,\n",
       " 'attention': 793,\n",
       " 'blocking': 794,\n",
       " 'interesting': 795,\n",
       " 'serious': 796,\n",
       " 'action': 797,\n",
       " 'criticism': 798,\n",
       " 'type': 799,\n",
       " 'upon': 800,\n",
       " 'statements': 801,\n",
       " '9': 802,\n",
       " 'fish': 803,\n",
       " \"wouldn't\": 804,\n",
       " 'hitler': 805,\n",
       " 'anonymous': 806,\n",
       " '12': 807,\n",
       " 'went': 808,\n",
       " 'ones': 809,\n",
       " 'attempt': 810,\n",
       " 'nonsense': 811,\n",
       " '11': 812,\n",
       " 'author': 813,\n",
       " 'explanation': 814,\n",
       " 'x': 815,\n",
       " 'homo': 816,\n",
       " 'meaning': 817,\n",
       " 'rights': 818,\n",
       " 'regards': 819,\n",
       " 'job': 820,\n",
       " 'improve': 821,\n",
       " 'goes': 822,\n",
       " 'previous': 823,\n",
       " 'mother': 824,\n",
       " 'context': 825,\n",
       " 'week': 826,\n",
       " 'soon': 827,\n",
       " 'conflict': 828,\n",
       " 'opinions': 829,\n",
       " 'teabag': 830,\n",
       " 'seriously': 831,\n",
       " 'disagree': 832,\n",
       " 'together': 833,\n",
       " 'record': 834,\n",
       " 'play': 835,\n",
       " 'tokenso': 836,\n",
       " 'email': 837,\n",
       " 'behavior': 838,\n",
       " 'science': 839,\n",
       " 'personally': 840,\n",
       " 'shall': 841,\n",
       " 'placed': 842,\n",
       " 'moved': 843,\n",
       " 'bastard': 844,\n",
       " 'works': 845,\n",
       " 'recently': 846,\n",
       " 'exist': 847,\n",
       " 'third': 848,\n",
       " 'generally': 849,\n",
       " 'prove': 850,\n",
       " 'cause': 851,\n",
       " 'boy': 852,\n",
       " 'citations': 853,\n",
       " 'external': 854,\n",
       " 'proper': 855,\n",
       " 'idiot': 856,\n",
       " \"tokeni'm\": 857,\n",
       " 'accept': 858,\n",
       " 'standard': 859,\n",
       " 'historical': 860,\n",
       " 'church': 861,\n",
       " 'series': 862,\n",
       " '24': 863,\n",
       " 'tokena': 864,\n",
       " 'available': 865,\n",
       " 'none': 866,\n",
       " 'accurate': 867,\n",
       " \"shouldn't\": 868,\n",
       " 'head': 869,\n",
       " 'level': 870,\n",
       " 'months': 871,\n",
       " 'heard': 872,\n",
       " '15': 873,\n",
       " 'complete': 874,\n",
       " 'tokenthank': 875,\n",
       " '1px': 876,\n",
       " 'rest': 877,\n",
       " 'decide': 878,\n",
       " 'legal': 879,\n",
       " 'description': 880,\n",
       " 'asking': 881,\n",
       " 'assume': 882,\n",
       " 'computer': 883,\n",
       " 'addition': 884,\n",
       " 'violation': 885,\n",
       " 'living': 886,\n",
       " 'culture': 887,\n",
       " 'size': 888,\n",
       " 'band': 889,\n",
       " 'doubt': 890,\n",
       " 'south': 891,\n",
       " 'lack': 892,\n",
       " 'tokenas': 893,\n",
       " 'sock': 894,\n",
       " 'wikiproject': 895,\n",
       " \"they're\": 896,\n",
       " 'copy': 897,\n",
       " 'gets': 898,\n",
       " 'contact': 899,\n",
       " 'de': 900,\n",
       " 'avoid': 901,\n",
       " 'run': 902,\n",
       " \"'\": 903,\n",
       " 'administrators': 904,\n",
       " 'buttsecks': 905,\n",
       " 'reported': 906,\n",
       " 'video': 907,\n",
       " 'situation': 908,\n",
       " 'sandbox': 909,\n",
       " 'necessary': 910,\n",
       " 'among': 911,\n",
       " 'sites': 912,\n",
       " 'speak': 913,\n",
       " 'difference': 914,\n",
       " 'outside': 915,\n",
       " 'abuse': 916,\n",
       " 'm': 917,\n",
       " 'data': 918,\n",
       " 'friends': 919,\n",
       " 'sections': 920,\n",
       " 'enjoy': 921,\n",
       " 'watch': 922,\n",
       " 'military': 923,\n",
       " 'friend': 924,\n",
       " 'million': 925,\n",
       " 'vandalize': 926,\n",
       " 'tokenthere': 927,\n",
       " 'member': 928,\n",
       " 'except': 929,\n",
       " 'debate': 930,\n",
       " 'username': 931,\n",
       " 'period': 932,\n",
       " 'space': 933,\n",
       " 'contributing': 934,\n",
       " 'supposed': 935,\n",
       " 'separate': 936,\n",
       " 'countries': 937,\n",
       " 'purpose': 938,\n",
       " 'india': 939,\n",
       " 'v': 940,\n",
       " 'meant': 941,\n",
       " 'multiple': 942,\n",
       " \"what's\": 943,\n",
       " 'bot': 944,\n",
       " 'early': 945,\n",
       " 'modern': 946,\n",
       " 'w': 947,\n",
       " 'fix': 948,\n",
       " 'apparently': 949,\n",
       " 'half': 950,\n",
       " 'messages': 951,\n",
       " 'team': 952,\n",
       " 'christian': 953,\n",
       " 'control': 954,\n",
       " 'usually': 955,\n",
       " 'civil': 956,\n",
       " 'takes': 957,\n",
       " 'details': 958,\n",
       " 'quality': 959,\n",
       " 'lost': 960,\n",
       " \"you'll\": 961,\n",
       " '14': 962,\n",
       " 'close': 963,\n",
       " 'fggt': 964,\n",
       " 'worked': 965,\n",
       " 'contribute': 966,\n",
       " 'happen': 967,\n",
       " 'allow': 968,\n",
       " 'court': 969,\n",
       " 'fucker': 970,\n",
       " 'special': 971,\n",
       " 'noobs': 972,\n",
       " 'tv': 973,\n",
       " 'warring': 974,\n",
       " 't': 975,\n",
       " 'born': 976,\n",
       " 'accepted': 977,\n",
       " 'coming': 978,\n",
       " 'face': 979,\n",
       " 'disruptive': 980,\n",
       " 'totally': 981,\n",
       " 'majority': 982,\n",
       " 'july': 983,\n",
       " 'refer': 984,\n",
       " 'gave': 985,\n",
       " 'giving': 986,\n",
       " 'population': 987,\n",
       " 'proposed': 988,\n",
       " 'standards': 989,\n",
       " 'absolutely': 990,\n",
       " 'worth': 991,\n",
       " 'sexual': 992,\n",
       " 'incorrect': 993,\n",
       " 'religion': 994,\n",
       " 'fan': 995,\n",
       " \"let's\": 996,\n",
       " 'wait': 997,\n",
       " 'couple': 998,\n",
       " 'merely': 999,\n",
       " 'significant': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text_eval):\n",
    "    sequences = tokenizer.texts_to_sequences([text_eval])\n",
    "    data = pad_sequences(sequences, maxlen=maxlen)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = preprocess_text(\"white card pankaj wq adqwd dqwwq \") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_1 = merged_comments[:training_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>96</td>\n",
       "      <td>2064</td>\n",
       "      <td>11</td>\n",
       "      <td>24</td>\n",
       "      <td>57</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>891</td>\n",
       "      <td>2264</td>\n",
       "      <td>1935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4795</td>\n",
       "      <td>815</td>\n",
       "      <td>442</td>\n",
       "      <td>1</td>\n",
       "      <td>1850</td>\n",
       "      <td>1230</td>\n",
       "      <td>103</td>\n",
       "      <td>820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>86</td>\n",
       "      <td>61</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>287</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>1077</td>\n",
       "      <td>1963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1209</td>\n",
       "      <td>37</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>729</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2048</td>\n",
       "      <td>24</td>\n",
       "      <td>62</td>\n",
       "      <td>7023</td>\n",
       "      <td>35</td>\n",
       "      <td>698</td>\n",
       "      <td>72</td>\n",
       "      <td>63</td>\n",
       "      <td>5876</td>\n",
       "      <td>13376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 150 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9    ...   140   141   142  \\\n",
       "0    0    0    0    0    0    0    0    0    0    0  ...    96  2064    11   \n",
       "1    0    0    0    0    0    0    0    0    0    0  ...     1     3  4795   \n",
       "2    0    0    0    0    0    0    0    0    0    0  ...     5    86    61   \n",
       "3    0    0    0    0    0    0    0    0    0    0  ...  1209    37    42   \n",
       "4    0    0    0    0    0    0    0    0    0    0  ...  2048    24    62   \n",
       "\n",
       "    143  144  145   146   147   148    149  \n",
       "0    24   57    6     1   891  2264   1935  \n",
       "1   815  442    1  1850  1230   103    820  \n",
       "2    20    1  287     5     8  1077   1963  \n",
       "3     1  729   17    11     2     4      9  \n",
       "4  7023   35  698    72    63  5876  13376  \n",
       "\n",
       "[5 rows x 150 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_1 = merged_comments[:training_samples]\n",
    "x_val_1 = merged_comments[training_samples: (training_samples + validation_samples)]\n",
    "x_test_1 = merged_comments[(training_samples + validation_samples) : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>rev_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>year</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>ns</th>\n",
       "      <th>sample</th>\n",
       "      <th>split</th>\n",
       "      <th>worker_id</th>\n",
       "      <th>quoting_attack</th>\n",
       "      <th>recipient_attack</th>\n",
       "      <th>third_party_attack</th>\n",
       "      <th>other_attack</th>\n",
       "      <th>attack</th>\n",
       "      <th>new_attack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>37675</td>\n",
       "      <td>`-NEWLINE_TOKENThis is not ``creative``.  Thos...</td>\n",
       "      <td>2002</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>9721</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>44816</td>\n",
       "      <td>`NEWLINE_TOKENNEWLINE_TOKEN:: the term ``stand...</td>\n",
       "      <td>2002</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>20234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>49851</td>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKENTrue or false, the s...</td>\n",
       "      <td>2002</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>26474</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>89320</td>\n",
       "      <td>Next, maybe you could work on being less cond...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>dev</td>\n",
       "      <td>26738</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>93890</td>\n",
       "      <td>This page will need disambiguation.</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>8010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  rev_id                                            comment  \\\n",
       "0           0   37675  `-NEWLINE_TOKENThis is not ``creative``.  Thos...   \n",
       "1           1   44816  `NEWLINE_TOKENNEWLINE_TOKEN:: the term ``stand...   \n",
       "2           2   49851  NEWLINE_TOKENNEWLINE_TOKENTrue or false, the s...   \n",
       "3           3   89320   Next, maybe you could work on being less cond...   \n",
       "4           4   93890               This page will need disambiguation.    \n",
       "\n",
       "   year  logged_in       ns  sample  split  worker_id  quoting_attack  \\\n",
       "0  2002      False  article  random  train       9721             0.0   \n",
       "1  2002      False  article  random  train      20234             0.0   \n",
       "2  2002      False  article  random  train      26474             0.0   \n",
       "3  2002       True  article  random    dev      26738             0.0   \n",
       "4  2002       True  article  random  train       8010             0.0   \n",
       "\n",
       "   recipient_attack  third_party_attack  other_attack  attack  new_attack  \n",
       "0                 0                 0.0           0.0     0.0           0  \n",
       "1                 0                 0.0           0.0     0.0           0  \n",
       "2                 0                 0.0           0.0     0.0           0  \n",
       "3                 1                 0.0           2.0     4.0           1  \n",
       "4                 0                 0.0           0.0     0.0           0  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "import tensorflow_hub as hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    " pred_vlaues = model.predict(preprocess_text(str(\"dark night sas\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.89783835, 0.09717047]], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_vlaues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/pankaj/anaconda/envs/nlp/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/pankaj/anaconda/envs/nlp/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "module_url = \"https://tfhub.dev/google/elmo/3\"\n",
    "embed = hub.load(module_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " running count 0\n",
      " running count 1\n",
      " running count 2\n",
      " running count 3\n",
      " running count 4\n",
      " running count 5\n",
      " running count 6\n",
      " running count 7\n",
      " running count 8\n",
      " running count 9\n",
      " running count 10\n"
     ]
    }
   ],
   "source": [
    "NAE_vals_array = np.empty((3,1024), float)\n",
    "# NAE_vals_array = []\n",
    "PAE_vals_array = np.empty((3,1024), float)\n",
    "\n",
    "for index, row in x_train_1.iterrows():\n",
    "    print(\" running count {}\".format(index))\n",
    "    text = row.comment.translate(str.maketrans('','',string.punctuation))\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens_without_sw = [word for word in tokens if not word in stopwords.words()]\n",
    "   \n",
    "    ### First Get the overall Classification\n",
    "    pred_vlaues = model.predict(preprocess_text(str(tokens_without_sw)))\n",
    "    classification = pred_vlaues.argmax()\n",
    "    \n",
    "    if classification == 0 and row.recipient_attack == 0:\n",
    "        mode = 'PAE' # Positive AutoEncoder\n",
    "    elif classification == 0 and row.recipient_attack == 1:\n",
    "        mode = 'NAE'\n",
    "    elif classification == 1 and row.recipient_attack == 0:\n",
    "        mode = 'PAE'\n",
    "    elif classification == 1 and row.recipient_attack == 1:\n",
    "        mode = 'NAE'\n",
    "    ### Get the Elmo vector\n",
    "    text=tf.convert_to_tensor([str(tokens_without_sw)])\n",
    "    out = embed.signatures['default'](text)['elmo']\n",
    "    pred_vals_array = np.empty((0,2), float)\n",
    "    \n",
    "    for i in range(len(tokens_without_sw)-2):\n",
    "        text_eval = tokens_without_sw[i:i+3]\n",
    "        a = preprocess_text(str(text_eval))\n",
    "        pred_vlaues = model.predict(a)\n",
    "        pred_vals_array = np.vstack((pred_vals_array,pred_vlaues))\n",
    "    if mode == \"NAE\":\n",
    "        # Only Negative Vals, and find the index of the highest contributing phrase\n",
    "        phrase_start_index = pred_vals_array[:,1].argmax()\n",
    "        ### Get the ELMO Encoding of the entire vector:\n",
    "        elmo_vecs = np.array(out[0][phrase_start_index:phrase_start_index+3])\n",
    "        NAE_vals_array = np.vstack((NAE_vals_array, elmo_vecs))\n",
    "    elif mode == \"PAE\":\n",
    "        # Only Positive Vals, and find the index of the highest contributing phrase\n",
    "        phrase_start_index = pred_vals_array[:,0].argmax()\n",
    "        PAE_vals_array = np.vstack((PAE_vals_array,out[0][phrase_start_index:phrase_start_index+3]))\n",
    "    if index == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "       ... \n",
       "145       6\n",
       "146       1\n",
       "147     891\n",
       "148    2264\n",
       "149    1935\n",
       "Name: 0, Length: 150, dtype: int32"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAE_vals_array = NAE_vals_array.reshape(int(NAE_vals_array.shape[0]/3),3,1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm autoencoder to recreate a timeseries\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import RepeatVector\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 16)          240000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                6272      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 247,394\n",
      "Trainable params: 247,394\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "timesteps = 3\n",
    "n_features = 1024\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "\n",
    "# define model\n",
    "model_TAE = Sequential()\n",
    "model_TAE .add(LSTM(1024, activation='relu', input_shape=(timesteps,n_features), return_sequences=True))\n",
    "model_TAE .add(LSTM(720, activation='relu', input_shape=(timesteps,n_features), return_sequences=True))\n",
    "model_TAE .add(LSTM(128, activation='relu', input_shape=(timesteps,n_features), return_sequences=True))\n",
    "model_TAE .add(LSTM(64, activation='relu', return_sequences=False))\n",
    "model_TAE .add(RepeatVector(timesteps))\n",
    "model_TAE .add(LSTM(64, activation='relu', return_sequences=True))\n",
    "model_TAE .add(LSTM(128, activation='relu', return_sequences=True))\n",
    "model_TAE .add(LSTM(720, activation='relu', return_sequences=True))\n",
    "model_TAE .add(LSTM(1027, activation='relu', return_sequences=True))\n",
    "model_TAE .add(TimeDistributed(Dense(n_features)))\n",
    "model_TAE .compile(optimizer='adam', loss='mse')\n",
    "model_TAE.summary()\n",
    "# model_TAE .fit(aa, aa, epochs=3, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 3, 1024)           8392704   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 3, 720)            5025600   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 3, 128)            434688    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 3, 64)             0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 3, 64)             33024     \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 3, 128)            98816     \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 3, 720)            2445120   \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 3, 1027)           7180784   \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 3, 1024)           1052672   \n",
      "=================================================================\n",
      "Total params: 24,712,816\n",
      "Trainable params: 24,712,816\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2 samples\n",
      "Epoch 1/3\n",
      "2/2 [==============================] - 1s 380ms/sample - loss: 2.2657\n",
      "Epoch 2/3\n",
      "2/2 [==============================] - 1s 477ms/sample - loss: 2.2649\n",
      "Epoch 3/3\n",
      "2/2 [==============================] - 1s 406ms/sample - loss: 2.2642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a466f7c90>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_TAE.summary()\n",
    "model_TAE.fit(NAE_vals_array, NAE_vals_array, epochs=3, batch_size=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
