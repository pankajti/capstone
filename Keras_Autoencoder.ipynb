{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Model. Ideally we will use some SOTA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import WhitespaceTokenizer as wst\n",
    "np.random.seed(7)\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File final.csv does not exist: 'final.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-99e64b73c7cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'final.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Unnamed: 0'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlp/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlp/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlp/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlp/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlp/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File final.csv does not exist: 'final.csv'"
     ]
    }
   ],
   "source": [
    "tweets = pd.read_csv('final.csv')\n",
    "tweets.drop(columns='Unnamed: 0',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>RT @NancyLeeGrahn: How did everyone feel about...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Positive</td>\n",
       "      <td>RT @ScottWalker: Didn't catch the full #GOPdeb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>RT @TJMShow: No mention of Tamir Rice and the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "      <td>RT @RobGeorge: That Carly Fiorina is trending ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive</td>\n",
       "      <td>RT @DanScavino: #GOPDebate w/ @realDonaldTrump...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                               text\n",
       "0   Neutral  RT @NancyLeeGrahn: How did everyone feel about...\n",
       "1  Positive  RT @ScottWalker: Didn't catch the full #GOPdeb...\n",
       "2   Neutral  RT @TJMShow: No mention of Tamir Rice and the ...\n",
       "3  Positive  RT @RobGeorge: That Carly Fiorina is trending ...\n",
       "4  Positive  RT @DanScavino: #GOPDebate w/ @realDonaldTrump..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Encoding the Sentiments \n",
    "\n",
    "tweets.loc[tweets.sentiment == \"Neutral\", 'Sentiment_C'] = int(0)\n",
    "tweets.loc[tweets.sentiment == \"Positive\", 'Sentiment_C'] = int(0)\n",
    "tweets.loc[tweets.sentiment == \"Negative\", 'Sentiment_C'] = int(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @warriorwoman91: I liked her and was happy when I heard she was going to be the moderator. Not anymore. #GOPDebate @megynkelly https://…'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.iloc[6].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text Cleaning \n",
    "\n",
    "# tweets.text = tweets.text.str.lower()\n",
    "\n",
    "tweets.text = tweets.text.str.replace(\"&amp\",\"\")\n",
    "tweets.text = tweets.text.str.replace(\"\\n\",\" \")\n",
    "tweets.text = tweets.text.str.replace(\"\\\\\\\\\",\" \")\n",
    "\n",
    "### Deleting Multiple Spaces \n",
    "\n",
    "tweets.text =  [re.sub(' +', ' ', str(x)) for x in tweets['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13871"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Reserve 1000 Tweets for Testing\n",
    "test_tweets = tweets.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([12520,  4509, 10923, 11469, 10989,   145,  9017, 13199,  6570,\n",
       "             8586,\n",
       "            ...\n",
       "            10936,  1712,  1557,  8282,  8875,  1366, 10344,  5821,  5737,\n",
       "             2958],\n",
       "           dtype='int64', length=1000)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tweets.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.drop(test_tweets.index,axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12871"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a common tweet corpus list\n",
    "\n",
    "all_tweets = tweets.text.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Tokenize and get unique tokens \n",
    "tokens = wst().tokenize(str(all_tweets))\n",
    "unique_tokens = list(set(tokens))\n",
    "unique_tokens.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "all_characters = list(str(all_tweets))\n",
    "l_sorted = Counter(all_characters).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Creating a Char Dictionary \n",
    "char_dict = dict()\n",
    "char_dict['<padding>'] = 0\n",
    "char_dict['<start>'] = 1\n",
    "char_dict['<unknown>'] = 2\n",
    "\n",
    "counter = 3\n",
    "\n",
    "#### Selected \n",
    "\n",
    "vocabulary_size = 200\n",
    "\n",
    "\n",
    "for i in range(int(vocabulary_size)):\n",
    "    char_dict[l_sorted[i][0]] = counter\n",
    "    counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Function to Create Character encodings\n",
    "def encode_tweets_char(string): \n",
    "    encoded_char_list = list()\n",
    "    encoded_char_list.append(\"1\")\n",
    "    for c in list(string):\n",
    "        try:\n",
    "            encoded_char_list.append(str((char_dict[c])))\n",
    "        except KeyError:\n",
    "            encoded_char_list.append(str(char_dict['<unknown>']))\n",
    "        \n",
    "    return encoded_char_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse dictionary to get words from numbers\n",
    "reverse_char_dict = dict(zip(char_dict.values(), char_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Decoding Function - To convert the encoded tweet back to character (sequence)\n",
    "def decode_tweets_char(list_enc): \n",
    "    for i in list_enc:\n",
    "        print(reverse_char_dict[int(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = encode_tweets_char(tweets.text[19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " '33',\n",
       " '31',\n",
       " '3',\n",
       " '34',\n",
       " '28',\n",
       " '6',\n",
       " '18',\n",
       " '4',\n",
       " '13',\n",
       " '6',\n",
       " '24',\n",
       " '4',\n",
       " '13',\n",
       " '13',\n",
       " '4',\n",
       " '11',\n",
       " '35',\n",
       " '3',\n",
       " '50',\n",
       " '15',\n",
       " '17',\n",
       " '36',\n",
       " '6',\n",
       " '16',\n",
       " '4',\n",
       " '4',\n",
       " '35',\n",
       " '3',\n",
       " '28',\n",
       " '6',\n",
       " '23',\n",
       " '8',\n",
       " '9',\n",
       " '22',\n",
       " '3',\n",
       " '32',\n",
       " '7',\n",
       " '11',\n",
       " '3',\n",
       " '5',\n",
       " '11',\n",
       " '6',\n",
       " '9',\n",
       " '10',\n",
       " '22',\n",
       " '4',\n",
       " '9',\n",
       " '14',\n",
       " '4',\n",
       " '11',\n",
       " '3',\n",
       " '10',\n",
       " '15',\n",
       " '11',\n",
       " '22',\n",
       " '4',\n",
       " '11',\n",
       " '23',\n",
       " '3',\n",
       " '32',\n",
       " '7',\n",
       " '11',\n",
       " '3',\n",
       " '10',\n",
       " '7',\n",
       " '13',\n",
       " '14',\n",
       " '8',\n",
       " '4',\n",
       " '11',\n",
       " '10',\n",
       " '26',\n",
       " '3',\n",
       " '10',\n",
       " '6',\n",
       " '8',\n",
       " '13',\n",
       " '7',\n",
       " '11',\n",
       " '10',\n",
       " '3',\n",
       " '6',\n",
       " '9',\n",
       " '14',\n",
       " '3',\n",
       " '6',\n",
       " '8',\n",
       " '11',\n",
       " '18',\n",
       " '4',\n",
       " '9',\n",
       " '3',\n",
       " '14',\n",
       " '7',\n",
       " '4',\n",
       " '10',\n",
       " '3',\n",
       " '9',\n",
       " '7',\n",
       " '5',\n",
       " '3',\n",
       " '18',\n",
       " '6',\n",
       " '36',\n",
       " '4',\n",
       " '3',\n",
       " '7',\n",
       " '15',\n",
       " '11',\n",
       " '3',\n",
       " '17',\n",
       " '7',\n",
       " '15',\n",
       " '9',\n",
       " '5',\n",
       " '11',\n",
       " '23',\n",
       " '3',\n",
       " '10',\n",
       " '6',\n",
       " '32',\n",
       " '4',\n",
       " '11',\n",
       " '3',\n",
       " '21',\n",
       " '50',\n",
       " '6',\n",
       " '3',\n",
       " '21',\n",
       " '24',\n",
       " '27',\n",
       " '28',\n",
       " '25',\n",
       " '4',\n",
       " '16',\n",
       " '6',\n",
       " '5',\n",
       " '4']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Encoding all the tweets \n",
    "\n",
    "tweets['Encoded'] = tweets.text.apply(encode_tweets_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tweets.Encoded.values\n",
    "y_train = tweets.Sentiment_C.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "# Selecting Max Character Limit\n",
    "character_limit = 151 \n",
    "\n",
    "# Padding all the sequences\n",
    "x_train = sequence.pad_sequences(x_train, padding = 'pre', maxlen=character_limit, truncating = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train_split, x_test_split, y_train_split, y_test_split = train_test_split(x_train, y_train, test_size = 0.15, random_state = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Converting Y to catagorical \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train_binary = to_categorical(y_train_split)\n",
    "y_test_binary = to_categorical(y_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout, LSTM, Bidirectional\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11790 samples, validate on 2081 samples\n",
      "Epoch 1/15\n",
      "11790/11790 [==============================] - 56s 5ms/sample - loss: 0.6898 - accuracy: 0.5696 - val_loss: 0.6633 - val_accuracy: 0.6112\n",
      "Epoch 2/15\n",
      "11790/11790 [==============================] - 28s 2ms/sample - loss: 0.6576 - accuracy: 0.6196 - val_loss: 0.6540 - val_accuracy: 0.6271\n",
      "Epoch 3/15\n",
      "11790/11790 [==============================] - 28s 2ms/sample - loss: 0.6497 - accuracy: 0.6267 - val_loss: 0.6493 - val_accuracy: 0.6295\n",
      "Epoch 4/15\n",
      "11790/11790 [==============================] - 28s 2ms/sample - loss: 0.6437 - accuracy: 0.6295 - val_loss: 0.6452 - val_accuracy: 0.6353\n",
      "Epoch 5/15\n",
      "11790/11790 [==============================] - 28s 2ms/sample - loss: 0.6384 - accuracy: 0.6295 - val_loss: 0.6424 - val_accuracy: 0.6324\n",
      "Epoch 6/15\n",
      "11790/11790 [==============================] - 28s 2ms/sample - loss: 0.6328 - accuracy: 0.6367 - val_loss: 0.6416 - val_accuracy: 0.6386\n",
      "Epoch 7/15\n",
      "11790/11790 [==============================] - 28s 2ms/sample - loss: 0.6287 - accuracy: 0.6389 - val_loss: 0.6376 - val_accuracy: 0.6430\n",
      "Epoch 8/15\n",
      "11790/11790 [==============================] - 28s 2ms/sample - loss: 0.6241 - accuracy: 0.6394 - val_loss: 0.6352 - val_accuracy: 0.6406\n",
      "Epoch 9/15\n",
      "11790/11790 [==============================] - 28s 2ms/sample - loss: 0.6200 - accuracy: 0.6436 - val_loss: 0.6337 - val_accuracy: 0.6377\n",
      "Epoch 10/15\n",
      "11790/11790 [==============================] - 28s 2ms/sample - loss: 0.6162 - accuracy: 0.6508 - val_loss: 0.6333 - val_accuracy: 0.6372\n",
      "Epoch 11/15\n",
      "11790/11790 [==============================] - 28s 2ms/sample - loss: 0.6127 - accuracy: 0.6524 - val_loss: 0.6312 - val_accuracy: 0.6391\n",
      "Epoch 12/15\n",
      "11790/11790 [==============================] - 28s 2ms/sample - loss: 0.6092 - accuracy: 0.6558 - val_loss: 0.6297 - val_accuracy: 0.6391\n",
      "Epoch 13/15\n",
      "11790/11790 [==============================] - 28s 2ms/sample - loss: 0.6061 - accuracy: 0.6592 - val_loss: 0.6280 - val_accuracy: 0.6410\n",
      "Epoch 14/15\n",
      "11790/11790 [==============================] - 28s 2ms/sample - loss: 0.6034 - accuracy: 0.6616 - val_loss: 0.6271 - val_accuracy: 0.6425\n",
      "Epoch 15/15\n",
      "11790/11790 [==============================] - 28s 2ms/sample - loss: 0.6004 - accuracy: 0.6673 - val_loss: 0.6258 - val_accuracy: 0.6439\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a56768890>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nadam = optimizers.Nadam(lr = 0.00001)\n",
    "\n",
    "model1=Sequential()\n",
    "model1.add(Embedding(vocabulary_size+3, 100 , input_length=character_limit,trainable = True))\n",
    "model1.add(SimpleRNN(128,unroll=True,return_sequences = True))\n",
    "model1.add(SimpleRNN(40,unroll=True))\n",
    "model1.add(Dense(2, activation='softmax'))\n",
    "model1.compile(loss = 'sparse_categorical_crossentropy', optimizer = Nadam, metrics = ['accuracy'])\n",
    "\n",
    "cb = [EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=2,\n",
    "                              verbose=0, mode='auto')]\n",
    "    \n",
    "model1.fit(x_train_split, y_train_split, validation_data=(x_test_split,y_test_split), batch_size=150, epochs=15,callbacks = cb ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.save('Tweets_Classifier.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment_C</th>\n",
       "      <th>Encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12520</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Damn... I'd turn Republican for this. #GOPDeba...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[1, 25, 6, 18, 9, 30, 30, 30, 3, 41, 19, 14, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4509</th>\n",
       "      <td>Negative</td>\n",
       "      <td>RT @bobcesca_go: As if the rest of the debate ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[1, 33, 31, 3, 34, 16, 7, 16, 17, 4, 10, 17, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10923</th>\n",
       "      <td>Negative</td>\n",
       "      <td>I would expect this farce from the BOTTOM-tier...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[1, 41, 3, 29, 7, 15, 13, 14, 3, 4, 49, 20, 4,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11469</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>Straight Outta Compton commercial on Fox News....</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1, 40, 5, 11, 6, 8, 22, 12, 5, 3, 27, 15, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10989</th>\n",
       "      <td>Negative</td>\n",
       "      <td>RT @RWSurferGirl: Is it just me or does anyone...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[1, 33, 31, 3, 34, 33, 42, 40, 15, 11, 32, 4, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment                                               text  \\\n",
       "12520  Negative  Damn... I'd turn Republican for this. #GOPDeba...   \n",
       "4509   Negative  RT @bobcesca_go: As if the rest of the debate ...   \n",
       "10923  Negative  I would expect this farce from the BOTTOM-tier...   \n",
       "11469   Neutral  Straight Outta Compton commercial on Fox News....   \n",
       "10989  Negative  RT @RWSurferGirl: Is it just me or does anyone...   \n",
       "\n",
       "       Sentiment_C                                            Encoded  \n",
       "12520          1.0  [1, 25, 6, 18, 9, 30, 30, 30, 3, 41, 19, 14, 3...  \n",
       "4509           1.0  [1, 33, 31, 3, 34, 16, 7, 16, 17, 4, 10, 17, 6...  \n",
       "10923          1.0  [1, 41, 3, 29, 7, 15, 13, 14, 3, 4, 49, 20, 4,...  \n",
       "11469          0.0  [1, 40, 5, 11, 6, 8, 22, 12, 5, 3, 27, 15, 5, ...  \n",
       "10989          1.0  [1, 33, 31, 3, 34, 33, 42, 40, 15, 11, 32, 4, ...  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function IteratorResourceDeleter.__del__ at 0x1a4ecf0b90>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shantanu/opt/anaconda3/envs/LEXACQ/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py\", line 541, in __del__\n",
      "    handle=self._handle, deleter=self._deleter)\n",
      "  File \"/Users/shantanu/opt/anaconda3/envs/LEXACQ/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_dataset_ops.py\", line 1157, in delete_iterator\n",
      "    \"DeleteIterator\", handle=handle, deleter=deleter, name=name)\n",
      "  File \"/Users/shantanu/opt/anaconda3/envs/LEXACQ/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 793, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"/Users/shantanu/opt/anaconda3/envs/LEXACQ/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 544, in create_op\n",
      "    inp = self.capture(inp)\n",
      "  File \"/Users/shantanu/opt/anaconda3/envs/LEXACQ/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 594, in capture\n",
      "    name = tensor.op.name\n",
      "  File \"/Users/shantanu/opt/anaconda3/envs/LEXACQ/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 1882, in name\n",
      "    return c_api.TF_OperationName(self._c_op)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xa4 in position 1: invalid start byte\n",
      "Exception ignored in: <function IteratorResourceDeleter.__del__ at 0x1a4ecf0b90>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shantanu/opt/anaconda3/envs/LEXACQ/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py\", line 541, in __del__\n",
      "    handle=self._handle, deleter=self._deleter)\n",
      "  File \"/Users/shantanu/opt/anaconda3/envs/LEXACQ/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_dataset_ops.py\", line 1157, in delete_iterator\n",
      "    \"DeleteIterator\", handle=handle, deleter=deleter, name=name)\n",
      "  File \"/Users/shantanu/opt/anaconda3/envs/LEXACQ/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 793, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"/Users/shantanu/opt/anaconda3/envs/LEXACQ/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 544, in create_op\n",
      "    inp = self.capture(inp)\n",
      "  File \"/Users/shantanu/opt/anaconda3/envs/LEXACQ/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 594, in capture\n",
      "    name = tensor.op.name\n",
      "  File \"/Users/shantanu/opt/anaconda3/envs/LEXACQ/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 1882, in name\n",
      "    return c_api.TF_OperationName(self._c_op)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xa4 in position 1: invalid start byte\n"
     ]
    }
   ],
   "source": [
    "model = models.load_model(\"Tweets_Classifier.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(eval_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_enc = encode_tweets_char(\"I liked her and was happy when I heard she was going to be the moderator. Not anymore\")\n",
    "eval_enc = sequence.pad_sequences([eval_enc], padding = 'pre', maxlen=character_limit, truncating = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_enc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.47523627, 0.5247637 ]], dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(eval_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Testing and Abalation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import string \n",
    "from nltk.corpus import stopwords \n",
    "#requires Tensorflow 2.0 and Tf-hub 0.7!!\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/shantanu/opt/anaconda3/envs/LEXACQ/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/shantanu/opt/anaconda3/envs/LEXACQ/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "module_url = \"https://tfhub.dev/google/elmo/3\"\n",
    "embed = hub.load(module_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=tf.convert_to_tensor([\"I am feeling kind of blue\", \"Blue is the color of the sky\",\"I am feeling kind of blue\"])\n",
    "out = embed.signatures['default'](text)['elmo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 7, 1024])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text_eval):\n",
    "    eval_enc = encode_tweets_char(text)\n",
    "    eval_enc = sequence.pad_sequences([eval_enc], padding = 'pre', maxlen=character_limit, truncating = 'post')\n",
    "    return eval_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweets.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment_C</th>\n",
       "      <th>Encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12520</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Damn... I'd turn Republican for this. #GOPDeba...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[1, 25, 6, 18, 9, 30, 30, 30, 3, 41, 19, 14, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4509</td>\n",
       "      <td>Negative</td>\n",
       "      <td>RT @bobcesca_go: As if the rest of the debate ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[1, 33, 31, 3, 34, 16, 7, 16, 17, 4, 10, 17, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10923</td>\n",
       "      <td>Negative</td>\n",
       "      <td>I would expect this farce from the BOTTOM-tier...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[1, 41, 3, 29, 7, 15, 13, 14, 3, 4, 49, 20, 4,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11469</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Straight Outta Compton commercial on Fox News....</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1, 40, 5, 11, 6, 8, 22, 12, 5, 3, 27, 15, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10989</td>\n",
       "      <td>Negative</td>\n",
       "      <td>RT @RWSurferGirl: Is it just me or does anyone...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[1, 33, 31, 3, 34, 33, 42, 40, 15, 11, 32, 4, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index sentiment                                               text  \\\n",
       "0  12520  Negative  Damn... I'd turn Republican for this. #GOPDeba...   \n",
       "1   4509  Negative  RT @bobcesca_go: As if the rest of the debate ...   \n",
       "2  10923  Negative  I would expect this farce from the BOTTOM-tier...   \n",
       "3  11469   Neutral  Straight Outta Compton commercial on Fox News....   \n",
       "4  10989  Negative  RT @RWSurferGirl: Is it just me or does anyone...   \n",
       "\n",
       "   Sentiment_C                                            Encoded  \n",
       "0          1.0  [1, 25, 6, 18, 9, 30, 30, 30, 3, 41, 19, 14, 3...  \n",
       "1          1.0  [1, 33, 31, 3, 34, 16, 7, 16, 17, 4, 10, 17, 6...  \n",
       "2          1.0  [1, 41, 3, 29, 7, 15, 13, 14, 3, 4, 49, 20, 4,...  \n",
       "3          0.0  [1, 40, 5, 11, 6, 8, 22, 12, 5, 3, 27, 15, 5, ...  \n",
       "4          1.0  [1, 33, 31, 3, 34, 33, 42, 40, 15, 11, 32, 4, ...  "
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B (3, 1024)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 3 and the array at index 1 has size 1024",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-300-5ad1094a6464>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m#             NAE_vals_array = np.dstack((NAE_vals_array, elmo_vecs))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNAE_vals_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melmo_vecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNAE_vals_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mappend\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/LEXACQ/lib/python3.7/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(arr, values, axis)\u001b[0m\n\u001b[1;32m   4691\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4692\u001b[0m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4693\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 3 and the array at index 1 has size 1024"
     ]
    }
   ],
   "source": [
    "### Abalation and Get the Vectors of the important trigrams \n",
    "\n",
    "pred_vals_array = np.empty((0,2), float)\n",
    "NAE_vals_array = np.empty((1,3,1024), float)\n",
    "# NAE_vals_array = []\n",
    "PAE_vals_array = np.empty((3,1024), float)\n",
    "\n",
    "for index, row in test_tweets.iterrows():\n",
    "    \n",
    "    text = row.text.translate(str.maketrans('','',string.punctuation))\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens_without_sw = [word for word in tokens if not word in stopwords.words()]\n",
    "    \n",
    "    ### First Get the overall Classification\n",
    "    \n",
    "    pred_vlaues = model.predict(preprocess_text(str(tokens_without_sw)))\n",
    "    \n",
    "    classification = pred_vlaues.argmax()\n",
    "    \n",
    "    if classification == 0 and row.Sentiment_C == 0:\n",
    "        mode = 'PAE' # Positive AutoEncoder\n",
    "    elif classification == 0 and row.Sentiment_C == 1:\n",
    "        mode = 'NAE'\n",
    "    elif classification == 1 and row.Sentiment_C == 0:\n",
    "        mode = 'PAE'\n",
    "    elif classification == 1 and row.Sentiment_C == 1:\n",
    "        mode = 'NAE'\n",
    "    \n",
    "    for i in range(len(tokens_without_sw)-2):\n",
    "        \n",
    "        text_eval = tokens_without_sw[i:i+3]\n",
    "\n",
    "        a = preprocess_text(str(text_eval))\n",
    "        pred_vlaues = model.predict(a)\n",
    "        pred_vals_array = np.vstack((pred_vals_array,pred_vlaues))\n",
    "        \n",
    "        if mode == \"NAE\":\n",
    "\n",
    "            # Only Negative Vals, and find the index of the highest contributing phrase\n",
    "\n",
    "            phrase_start_index = pred_vals_array[:,1].argmax()\n",
    "\n",
    "            ### Get the ELMO Encoding of the entire vector:\n",
    "\n",
    "            text_elmo = row.text\n",
    "\n",
    "            text=tf.convert_to_tensor([str(tokens_without_sw)])\n",
    "            out = embed.signatures['default'](text)['elmo']\n",
    "\n",
    "            elmo_vecs = np.array(out[0][phrase_start_index:phrase_start_index+3])\n",
    "\n",
    "            print(\"B\",elmo_vecs.shape)\n",
    "            \n",
    "            \n",
    "#             NAE_vals_array = np.dstack((NAE_vals_array, elmo_vecs))\n",
    "            \n",
    "            print(np.append(NAE_vals_array, np.atleast_3d(elmo_vecs), axis=0).shape)\n",
    "            \n",
    "            print(\"\",NAE_vals_array.shape)\n",
    "\n",
    "#             NAE_vals_array.append(elmo_vecs)\n",
    "\n",
    "        elif mode == \"PAE\":\n",
    "\n",
    "            # Only Positive Vals, and find the index of the highest contributing phrase\n",
    "\n",
    "            phrase_start_index = pred_vals_array[:,0].argmax()\n",
    "\n",
    "            ### Get the ELMO Encoding of the entire vector:\n",
    "\n",
    "            text_elmo = row.text\n",
    "\n",
    "            text=tf.convert_to_tensor([str(tokens_without_sw)])\n",
    "            out = embed.signatures['default'](text)['elmo']\n",
    "\n",
    "            PAE_vals_array = np.vstack((PAE_vals_array,out[0][phrase_start_index:phrase_start_index+3]))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81, 1024)"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NAE_vals_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.0"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(int(NAE_vals_array.shape[0])/int(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = NAE_vals_array.reshape(int(NAE_vals_array.shape[0]/3),3,1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 1024)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1024)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NAE_vals_array[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = np.array(NAE_vals_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.73210824, -0.3453857 , -0.124824  , ..., -0.18160583,\n",
       "         0.4167027 ,  0.24779648],\n",
       "       [-0.21743327, -0.1413041 , -0.01641104, ...,  0.09404293,\n",
       "         0.06934943, -0.25008377],\n",
       "       [-0.15964414, -0.01390902, -0.16066319, ...,  0.564662  ,\n",
       "         0.42623278,  0.57753956]], dtype=float32)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=85024, shape=(2, 1024), dtype=float32, numpy=\n",
       "array([[-0.73210824, -0.3453857 , -0.124824  , ..., -0.18160583,\n",
       "         0.4167027 ,  0.24779648],\n",
       "       [-0.21743327, -0.1413041 , -0.01641104, ...,  0.09404293,\n",
       "         0.06934943, -0.25008377]], dtype=float32)>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.63093668, 0.36906329],\n",
       "       [0.7150175 , 0.28498253],\n",
       "       [0.7244342 , 0.2755658 ],\n",
       "       [0.67767799, 0.32232198]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_vals_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm autoencoder to recreate a timeseries\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import RepeatVector\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A UDF to convert input data into 3-D\n",
    "array as required for LSTM network.\n",
    "'''\n",
    "\n",
    "def temporalize(X, y, lookback):\n",
    "    output_X = []\n",
    "    output_y = []\n",
    "    for i in range(len(X)-lookback-1):\n",
    "        t = []\n",
    "        for j in range(1,lookback+1):\n",
    "            # Gather past records upto the lookback period\n",
    "            t.append(X[[(i+j+1)], :])\n",
    "        output_X.append(t)\n",
    "        output_y.append(y[i+lookback+1])\n",
    "    return output_X, output_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1  , 0.001],\n",
       "       [0.2  , 0.008],\n",
       "       [0.3  , 0.027],\n",
       "       [0.4  , 0.064],\n",
       "       [0.5  , 0.125],\n",
       "       [0.6  , 0.216],\n",
       "       [0.7  , 0.343],\n",
       "       [0.8  , 0.512],\n",
       "       [0.9  , 0.729]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define input timeseries\n",
    "timeseries = np.array([[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "                       [0.1**3, 0.2**3, 0.3**3, 0.4**3, 0.5**3, 0.6**3, 0.7**3, 0.8**3, 0.9**3]]).transpose()\n",
    "\n",
    "timesteps = timeseries.shape[0]\n",
    "n_features = timeseries.shape[1]\n",
    "timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.3  , 0.027],\n",
       "        [0.4  , 0.064],\n",
       "        [0.5  , 0.125]],\n",
       "\n",
       "       [[0.4  , 0.064],\n",
       "        [0.5  , 0.125],\n",
       "        [0.6  , 0.216]],\n",
       "\n",
       "       [[0.5  , 0.125],\n",
       "        [0.6  , 0.216],\n",
       "        [0.7  , 0.343]],\n",
       "\n",
       "       [[0.6  , 0.216],\n",
       "        [0.7  , 0.343],\n",
       "        [0.8  , 0.512]],\n",
       "\n",
       "       [[0.7  , 0.343],\n",
       "        [0.8  , 0.512],\n",
       "        [0.9  , 0.729]]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timesteps = 3\n",
    "X, y = temporalize(X = timeseries, y = np.zeros(len(timeseries)), lookback = timesteps)\n",
    "\n",
    "n_features = 2\n",
    "X = np.array(X)\n",
    "X = X.reshape(X.shape[0], timesteps, n_features)\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_24 (LSTM)               (None, 3, 1024)           4206592   \n",
      "_________________________________________________________________\n",
      "lstm_25 (LSTM)               (None, 3, 720)            5025600   \n",
      "_________________________________________________________________\n",
      "lstm_26 (LSTM)               (None, 200)               736800    \n",
      "_________________________________________________________________\n",
      "repeat_vector_6 (RepeatVecto (None, 3, 200)            0         \n",
      "_________________________________________________________________\n",
      "lstm_27 (LSTM)               (None, 3, 200)            320800    \n",
      "_________________________________________________________________\n",
      "lstm_28 (LSTM)               (None, 3, 720)            2652480   \n",
      "_________________________________________________________________\n",
      "lstm_29 (LSTM)               (None, 3, 1024)           7147520   \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 3, 2)              2050      \n",
      "=================================================================\n",
      "Total params: 20,091,842\n",
      "Trainable params: 20,091,842\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(1024, activation='relu', input_shape=(timesteps,n_features), return_sequences=True))\n",
    "model.add(LSTM(720, activation='relu', return_sequences=True))\n",
    "model.add(LSTM(200, activation='relu', return_sequences=False))\n",
    "model.add(RepeatVector(timesteps))\n",
    "model.add(LSTM(200, activation='relu', return_sequences=True))\n",
    "model.add(LSTM(720, activation='relu', return_sequences=True))\n",
    "model.add(LSTM(1024, activation='relu', return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(n_features)))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5 samples\n",
      "Epoch 1/300\n",
      "5/5 [==============================] - 10s 2s/sample - loss: 0.2468\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 78ms/sample - loss: 0.2427\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 78ms/sample - loss: 0.2371\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 79ms/sample - loss: 0.2303\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 81ms/sample - loss: 0.2219\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 76ms/sample - loss: 0.2114\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 81ms/sample - loss: 0.1985\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 77ms/sample - loss: 0.1827\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 75ms/sample - loss: 0.1634\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 0s 82ms/sample - loss: 0.1407\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 0s 84ms/sample - loss: 0.1147\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 86ms/sample - loss: 0.0876\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 0s 85ms/sample - loss: 0.0658\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 87ms/sample - loss: 0.0671\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 0s 82ms/sample - loss: 0.0911\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 83ms/sample - loss: 0.0873\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 85ms/sample - loss: 0.0720\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 82ms/sample - loss: 0.0611\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 82ms/sample - loss: 0.0572\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 86ms/sample - loss: 0.0575\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 91ms/sample - loss: 0.0594\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 93ms/sample - loss: 0.0610\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 0s 87ms/sample - loss: 0.0617\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 92ms/sample - loss: 0.0613\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 92ms/sample - loss: 0.0598\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 93ms/sample - loss: 0.0575\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 0s 88ms/sample - loss: 0.0549\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 0s 87ms/sample - loss: 0.0524\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 92ms/sample - loss: 0.0503\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 87ms/sample - loss: 0.0489\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 87ms/sample - loss: 0.0483\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 91ms/sample - loss: 0.0482\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 86ms/sample - loss: 0.0482\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 85ms/sample - loss: 0.0478\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 84ms/sample - loss: 0.0469\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 0s 84ms/sample - loss: 0.0456\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 0s 88ms/sample - loss: 0.0440\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 0s 87ms/sample - loss: 0.0426\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 87ms/sample - loss: 0.0415\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 89ms/sample - loss: 0.0408\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 88ms/sample - loss: 0.0402\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 90ms/sample - loss: 0.0398\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 85ms/sample - loss: 0.0392\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 0s 91ms/sample - loss: 0.0385\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 89ms/sample - loss: 0.0376\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 89ms/sample - loss: 0.0365\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 88ms/sample - loss: 0.0355\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 90ms/sample - loss: 0.0345\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 0s 86ms/sample - loss: 0.0336\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 87ms/sample - loss: 0.0328\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 87ms/sample - loss: 0.0321\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 85ms/sample - loss: 0.0314\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 87ms/sample - loss: 0.0305\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 84ms/sample - loss: 0.0295\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 84ms/sample - loss: 0.0284\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 85ms/sample - loss: 0.0273\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 86ms/sample - loss: 0.0263\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 85ms/sample - loss: 0.0252\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 89ms/sample - loss: 0.0240\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 88ms/sample - loss: 0.0225\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 83ms/sample - loss: 0.0210\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 0s 85ms/sample - loss: 0.0196\n",
      "Epoch 63/300\n",
      "5/5 [==============================] - 0s 88ms/sample - loss: 0.0182\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 84ms/sample - loss: 0.0165\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 84ms/sample - loss: 0.0148\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 85ms/sample - loss: 0.0136\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 83ms/sample - loss: 0.0124\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 84ms/sample - loss: 0.0112\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 0s 88ms/sample - loss: 0.0095\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 90ms/sample - loss: 0.0080\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 0s 89ms/sample - loss: 0.0069\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 86ms/sample - loss: 0.0065\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 94ms/sample - loss: 0.0062\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 93ms/sample - loss: 0.0061\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 0s 93ms/sample - loss: 0.0058\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 88ms/sample - loss: 0.0052\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 84ms/sample - loss: 0.0046\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 93ms/sample - loss: 0.0039\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 88ms/sample - loss: 0.0033\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 0s 90ms/sample - loss: 0.0028\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 86ms/sample - loss: 0.0023\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 87ms/sample - loss: 0.0020\n",
      "Epoch 83/300\n",
      "5/5 [==============================] - 0s 89ms/sample - loss: 0.0019\n",
      "Epoch 84/300\n",
      "5/5 [==============================] - 0s 85ms/sample - loss: 0.0019\n",
      "Epoch 85/300\n",
      "5/5 [==============================] - 0s 86ms/sample - loss: 0.0019\n",
      "Epoch 86/300\n",
      "5/5 [==============================] - 0s 84ms/sample - loss: 0.0019\n",
      "Epoch 87/300\n",
      "5/5 [==============================] - 0s 91ms/sample - loss: 0.0018\n",
      "Epoch 88/300\n",
      "5/5 [==============================] - 0s 87ms/sample - loss: 0.0015\n",
      "Epoch 89/300\n",
      "5/5 [==============================] - 0s 92ms/sample - loss: 0.0013\n",
      "Epoch 90/300\n",
      "5/5 [==============================] - 0s 90ms/sample - loss: 0.0011\n",
      "Epoch 91/300\n",
      "5/5 [==============================] - 0s 89ms/sample - loss: 9.5445e-04\n",
      "Epoch 92/300\n",
      "5/5 [==============================] - 0s 91ms/sample - loss: 9.1501e-04\n",
      "Epoch 93/300\n",
      "5/5 [==============================] - 0s 93ms/sample - loss: 8.9506e-04\n",
      "Epoch 94/300\n",
      "5/5 [==============================] - 0s 89ms/sample - loss: 8.7360e-04\n",
      "Epoch 95/300\n",
      "5/5 [==============================] - 0s 92ms/sample - loss: 8.4720e-04\n",
      "Epoch 96/300\n",
      "5/5 [==============================] - 0s 94ms/sample - loss: 7.8451e-04\n",
      "Epoch 97/300\n",
      "5/5 [==============================] - 0s 88ms/sample - loss: 7.3160e-04\n",
      "Epoch 98/300\n",
      "5/5 [==============================] - 0s 87ms/sample - loss: 6.7306e-04\n",
      "Epoch 99/300\n",
      "5/5 [==============================] - 0s 87ms/sample - loss: 6.4270e-04\n",
      "Epoch 100/300\n",
      "5/5 [==============================] - 0s 88ms/sample - loss: 6.2799e-04\n",
      "Epoch 101/300\n",
      "5/5 [==============================] - 0s 86ms/sample - loss: 6.1410e-04\n",
      "Epoch 102/300\n",
      "5/5 [==============================] - 0s 89ms/sample - loss: 5.9139e-04\n",
      "Epoch 103/300\n",
      "5/5 [==============================] - 0s 87ms/sample - loss: 5.4320e-04\n",
      "Epoch 104/300\n",
      "5/5 [==============================] - 0s 88ms/sample - loss: 4.9040e-04\n",
      "Epoch 105/300\n",
      "5/5 [==============================] - 0s 89ms/sample - loss: 4.3504e-04\n",
      "Epoch 106/300\n",
      "5/5 [==============================] - 0s 84ms/sample - loss: 3.9990e-04\n",
      "Epoch 107/300\n",
      "5/5 [==============================] - 0s 85ms/sample - loss: 3.7773e-04\n",
      "Epoch 108/300\n",
      "5/5 [==============================] - 0s 88ms/sample - loss: 3.6845e-04\n",
      "Epoch 109/300\n",
      "5/5 [==============================] - 0s 86ms/sample - loss: 3.5858e-04\n",
      "Epoch 110/300\n",
      "5/5 [==============================] - 0s 89ms/sample - loss: 3.4404e-04\n",
      "Epoch 111/300\n",
      "5/5 [==============================] - 0s 89ms/sample - loss: 3.2521e-04\n",
      "Epoch 112/300\n",
      "5/5 [==============================] - 0s 89ms/sample - loss: 3.0408e-04\n",
      "Epoch 113/300\n",
      "5/5 [==============================] - 0s 92ms/sample - loss: 2.8951e-04\n",
      "Epoch 114/300\n",
      "5/5 [==============================] - 0s 87ms/sample - loss: 2.8031e-04\n",
      "Epoch 115/300\n",
      "5/5 [==============================] - 0s 91ms/sample - loss: 2.7666e-04\n",
      "Epoch 116/300\n",
      "5/5 [==============================] - 1s 103ms/sample - loss: 2.6963e-04\n",
      "Epoch 117/300\n",
      "5/5 [==============================] - 1s 100ms/sample - loss: 2.6018e-04\n",
      "Epoch 118/300\n",
      "5/5 [==============================] - 0s 91ms/sample - loss: 2.4951e-04\n",
      "Epoch 119/300\n",
      "5/5 [==============================] - 0s 94ms/sample - loss: 2.4232e-04\n",
      "Epoch 120/300\n",
      "5/5 [==============================] - 0s 92ms/sample - loss: 2.3929e-04\n",
      "Epoch 121/300\n",
      "5/5 [==============================] - 0s 98ms/sample - loss: 2.3638e-04\n",
      "Epoch 122/300\n",
      "5/5 [==============================] - 1s 100ms/sample - loss: 2.3210e-04\n",
      "Epoch 123/300\n",
      "5/5 [==============================] - 0s 96ms/sample - loss: 2.2326e-04\n",
      "Epoch 124/300\n",
      "5/5 [==============================] - 1s 105ms/sample - loss: 2.1240e-04\n",
      "Epoch 125/300\n",
      "5/5 [==============================] - 0s 97ms/sample - loss: 2.0140e-04\n",
      "Epoch 126/300\n",
      "5/5 [==============================] - 1s 103ms/sample - loss: 1.9269e-04\n",
      "Epoch 127/300\n",
      "5/5 [==============================] - 1s 103ms/sample - loss: 1.8696e-04\n",
      "Epoch 128/300\n",
      "5/5 [==============================] - 1s 101ms/sample - loss: 1.8167e-04\n",
      "Epoch 129/300\n",
      "5/5 [==============================] - 1s 101ms/sample - loss: 1.7640e-04\n",
      "Epoch 130/300\n",
      "5/5 [==============================] - 0s 97ms/sample - loss: 1.7016e-04\n",
      "Epoch 131/300\n",
      "5/5 [==============================] - 0s 91ms/sample - loss: 1.6361e-04\n",
      "Epoch 132/300\n",
      "5/5 [==============================] - 0s 92ms/sample - loss: 1.5813e-04\n",
      "Epoch 133/300\n",
      "5/5 [==============================] - 0s 92ms/sample - loss: 1.5342e-04\n",
      "Epoch 134/300\n",
      "5/5 [==============================] - 0s 99ms/sample - loss: 1.4907e-04\n",
      "Epoch 135/300\n",
      "5/5 [==============================] - 1s 102ms/sample - loss: 1.4444e-04\n",
      "Epoch 136/300\n",
      "5/5 [==============================] - 0s 93ms/sample - loss: 1.3933e-04\n",
      "Epoch 137/300\n",
      "5/5 [==============================] - 0s 92ms/sample - loss: 1.3443e-04\n",
      "Epoch 138/300\n",
      "5/5 [==============================] - 0s 95ms/sample - loss: 1.2985e-04\n",
      "Epoch 139/300\n",
      "5/5 [==============================] - 0s 93ms/sample - loss: 1.2573e-04\n",
      "Epoch 140/300\n",
      "5/5 [==============================] - 0s 94ms/sample - loss: 1.2201e-04\n",
      "Epoch 141/300\n",
      "5/5 [==============================] - 0s 96ms/sample - loss: 1.1799e-04\n",
      "Epoch 142/300\n",
      "5/5 [==============================] - 0s 91ms/sample - loss: 1.1339e-04\n",
      "Epoch 143/300\n",
      "5/5 [==============================] - 0s 90ms/sample - loss: 1.0869e-04\n",
      "Epoch 144/300\n",
      "5/5 [==============================] - 0s 87ms/sample - loss: 1.0414e-04\n",
      "Epoch 145/300\n",
      "5/5 [==============================] - 0s 89ms/sample - loss: 9.9976e-05\n",
      "Epoch 146/300\n",
      "5/5 [==============================] - 0s 90ms/sample - loss: 9.6349e-05\n",
      "Epoch 147/300\n",
      "5/5 [==============================] - 0s 87ms/sample - loss: 9.2700e-05\n",
      "Epoch 148/300\n",
      "5/5 [==============================] - 0s 86ms/sample - loss: 8.8982e-05\n",
      "Epoch 149/300\n",
      "5/5 [==============================] - 0s 88ms/sample - loss: 8.5311e-05\n",
      "Epoch 150/300\n",
      "5/5 [==============================] - 0s 85ms/sample - loss: 8.1888e-05\n",
      "Epoch 151/300\n",
      "5/5 [==============================] - 0s 86ms/sample - loss: 7.8651e-05\n",
      "Epoch 152/300\n",
      "5/5 [==============================] - 0s 87ms/sample - loss: 7.5605e-05\n",
      "Epoch 153/300\n",
      "5/5 [==============================] - 0s 86ms/sample - loss: 7.2593e-05\n",
      "Epoch 154/300\n",
      "5/5 [==============================] - 0s 89ms/sample - loss: 6.9664e-05\n",
      "Epoch 155/300\n",
      "5/5 [==============================] - 0s 87ms/sample - loss: 6.6762e-05\n",
      "Epoch 156/300\n",
      "5/5 [==============================] - 0s 89ms/sample - loss: 6.3918e-05\n",
      "Epoch 157/300\n",
      "5/5 [==============================] - 0s 88ms/sample - loss: 6.1110e-05\n",
      "Epoch 158/300\n",
      "5/5 [==============================] - 0s 85ms/sample - loss: 5.8416e-05\n",
      "Epoch 159/300\n",
      "5/5 [==============================] - 0s 91ms/sample - loss: 5.5750e-05\n",
      "Epoch 160/300\n",
      "5/5 [==============================] - 0s 87ms/sample - loss: 5.3196e-05\n",
      "Epoch 161/300\n",
      "5/5 [==============================] - 0s 87ms/sample - loss: 5.0734e-05\n",
      "Epoch 162/300\n",
      "5/5 [==============================] - 0s 87ms/sample - loss: 4.8275e-05\n",
      "Epoch 163/300\n",
      "5/5 [==============================] - 0s 85ms/sample - loss: 4.5972e-05\n",
      "Epoch 164/300\n",
      "5/5 [==============================] - 0s 84ms/sample - loss: 4.3777e-05\n",
      "Epoch 165/300\n",
      "5/5 [==============================] - 0s 85ms/sample - loss: 4.1634e-05\n",
      "Epoch 166/300\n",
      "5/5 [==============================] - 0s 87ms/sample - loss: 3.9520e-05\n",
      "Epoch 167/300\n",
      "5/5 [==============================] - 0s 85ms/sample - loss: 3.7505e-05\n",
      "Epoch 168/300\n",
      "5/5 [==============================] - 0s 85ms/sample - loss: 3.5554e-05\n",
      "Epoch 169/300\n",
      "5/5 [==============================] - 0s 90ms/sample - loss: 3.3710e-05\n",
      "Epoch 170/300\n",
      "5/5 [==============================] - 0s 87ms/sample - loss: 3.1966e-05\n",
      "Epoch 171/300\n",
      "5/5 [==============================] - 0s 91ms/sample - loss: 3.0308e-05\n",
      "Epoch 172/300\n",
      "5/5 [==============================] - 0s 90ms/sample - loss: 2.8749e-05\n",
      "Epoch 173/300\n",
      "5/5 [==============================] - 0s 88ms/sample - loss: 2.7373e-05\n",
      "Epoch 174/300\n",
      "5/5 [==============================] - 0s 89ms/sample - loss: 2.6420e-05\n",
      "Epoch 175/300\n",
      "5/5 [==============================] - 0s 84ms/sample - loss: 2.6440e-05\n",
      "Epoch 176/300\n",
      "5/5 [==============================] - 0s 86ms/sample - loss: 2.9170e-05\n",
      "Epoch 177/300\n",
      "5/5 [==============================] - 0s 87ms/sample - loss: 3.4530e-05\n",
      "Epoch 178/300\n",
      "5/5 [==============================] - 0s 85ms/sample - loss: 4.0927e-05\n",
      "Epoch 179/300\n",
      "5/5 [==============================] - 0s 83ms/sample - loss: 3.4447e-05\n",
      "Epoch 180/300\n",
      "5/5 [==============================] - 0s 91ms/sample - loss: 2.2625e-05\n",
      "Epoch 181/300\n",
      "5/5 [==============================] - 0s 87ms/sample - loss: 1.8376e-05\n",
      "Epoch 182/300\n",
      "5/5 [==============================] - 0s 90ms/sample - loss: 2.4218e-05\n",
      "Epoch 183/300\n",
      "5/5 [==============================] - 0s 87ms/sample - loss: 2.7807e-05\n",
      "Epoch 184/300\n",
      "5/5 [==============================] - 0s 85ms/sample - loss: 2.1332e-05\n",
      "Epoch 185/300\n",
      "5/5 [==============================] - 0s 84ms/sample - loss: 1.5079e-05\n",
      "Epoch 186/300\n",
      "5/5 [==============================] - 0s 89ms/sample - loss: 1.6486e-05\n",
      "Epoch 187/300\n",
      "5/5 [==============================] - 0s 86ms/sample - loss: 2.0177e-05\n",
      "Epoch 188/300\n",
      "5/5 [==============================] - 0s 91ms/sample - loss: 1.8698e-05\n",
      "Epoch 189/300\n",
      "5/5 [==============================] - 0s 92ms/sample - loss: 1.3549e-05\n",
      "Epoch 190/300\n",
      "5/5 [==============================] - 0s 86ms/sample - loss: 1.2339e-05\n",
      "Epoch 191/300\n",
      "5/5 [==============================] - 0s 85ms/sample - loss: 1.4918e-05\n",
      "Epoch 192/300\n",
      "5/5 [==============================] - 0s 89ms/sample - loss: 1.5495e-05\n",
      "Epoch 193/300\n",
      "5/5 [==============================] - 0s 88ms/sample - loss: 1.2768e-05\n",
      "Epoch 194/300\n",
      "5/5 [==============================] - 0s 90ms/sample - loss: 1.0180e-05\n",
      "Epoch 195/300\n",
      "5/5 [==============================] - 0s 87ms/sample - loss: 1.0723e-05\n",
      "Epoch 196/300\n",
      "5/5 [==============================] - 0s 92ms/sample - loss: 1.2291e-05\n",
      "Epoch 197/300\n",
      "5/5 [==============================] - 0s 88ms/sample - loss: 1.1718e-05\n",
      "Epoch 198/300\n",
      "5/5 [==============================] - 0s 89ms/sample - loss: 9.6424e-06\n",
      "Epoch 199/300\n",
      "5/5 [==============================] - 0s 86ms/sample - loss: 8.3824e-06\n",
      "Epoch 200/300\n",
      "5/5 [==============================] - 0s 88ms/sample - loss: 8.9030e-06\n",
      "Epoch 201/300\n",
      "5/5 [==============================] - 0s 85ms/sample - loss: 9.7369e-06\n",
      "Epoch 202/300\n",
      "5/5 [==============================] - 0s 88ms/sample - loss: 9.3180e-06\n",
      "Epoch 203/300\n",
      "5/5 [==============================] - 0s 83ms/sample - loss: 7.9932e-06\n",
      "Epoch 204/300\n",
      "5/5 [==============================] - 0s 88ms/sample - loss: 7.0392e-06\n",
      "Epoch 205/300\n",
      "5/5 [==============================] - 0s 90ms/sample - loss: 7.1174e-06\n",
      "Epoch 206/300\n",
      "5/5 [==============================] - 0s 90ms/sample - loss: 7.6055e-06\n",
      "Epoch 207/300\n",
      "5/5 [==============================] - 0s 90ms/sample - loss: 7.5748e-06\n",
      "Epoch 208/300\n",
      "5/5 [==============================] - 0s 85ms/sample - loss: 6.8627e-06\n",
      "Epoch 209/300\n",
      "5/5 [==============================] - 0s 88ms/sample - loss: 6.0572e-06\n",
      "Epoch 210/300\n",
      "5/5 [==============================] - 0s 87ms/sample - loss: 5.7673e-06\n",
      "Epoch 211/300\n",
      "5/5 [==============================] - 0s 86ms/sample - loss: 5.9496e-06\n",
      "Epoch 212/300\n",
      "5/5 [==============================] - 0s 88ms/sample - loss: 6.1257e-06\n",
      "Epoch 213/300\n",
      "5/5 [==============================] - 0s 87ms/sample - loss: 5.9778e-06\n",
      "Epoch 214/300\n",
      "5/5 [==============================] - 0s 88ms/sample - loss: 5.4976e-06\n",
      "Epoch 215/300\n",
      "5/5 [==============================] - 0s 90ms/sample - loss: 4.9917e-06\n",
      "Epoch 216/300\n",
      "5/5 [==============================] - 0s 88ms/sample - loss: 4.7085e-06\n",
      "Epoch 217/300\n",
      "5/5 [==============================] - 0s 89ms/sample - loss: 4.6888e-06\n",
      "Epoch 218/300\n",
      "5/5 [==============================] - 0s 87ms/sample - loss: 4.7733e-06\n",
      "Epoch 219/300\n",
      "5/5 [==============================] - 0s 90ms/sample - loss: 4.7773e-06\n",
      "Epoch 220/300\n",
      "5/5 [==============================] - 0s 84ms/sample - loss: 4.6199e-06\n",
      "Epoch 221/300\n",
      "5/5 [==============================] - 0s 87ms/sample - loss: 4.3309e-06\n",
      "Epoch 222/300\n",
      "5/5 [==============================] - 0s 83ms/sample - loss: 4.0246e-06\n",
      "Epoch 223/300\n",
      "5/5 [==============================] - 0s 83ms/sample - loss: 3.7921e-06\n",
      "Epoch 224/300\n",
      "5/5 [==============================] - 0s 82ms/sample - loss: 3.6729e-06\n",
      "Epoch 225/300\n",
      "5/5 [==============================] - 0s 85ms/sample - loss: 3.6385e-06\n",
      "Epoch 226/300\n",
      "5/5 [==============================] - 0s 90ms/sample - loss: 3.6362e-06\n",
      "Epoch 227/300\n",
      "5/5 [==============================] - 1s 103ms/sample - loss: 3.6133e-06\n",
      "Epoch 228/300\n",
      "5/5 [==============================] - 0s 91ms/sample - loss: 3.5374e-06\n",
      "Epoch 229/300\n",
      "5/5 [==============================] - 0s 90ms/sample - loss: 3.4064e-06\n",
      "Epoch 230/300\n",
      "5/5 [==============================] - 0s 89ms/sample - loss: 3.2416e-06\n",
      "Epoch 231/300\n",
      "5/5 [==============================] - 0s 89ms/sample - loss: 3.0941e-06\n",
      "Epoch 232/300\n",
      "5/5 [==============================] - 0s 92ms/sample - loss: 2.9596e-06\n",
      "Epoch 233/300\n",
      "5/5 [==============================] - 0s 88ms/sample - loss: 2.8416e-06\n",
      "Epoch 234/300\n",
      "5/5 [==============================] - 0s 93ms/sample - loss: 2.7606e-06\n",
      "Epoch 235/300\n",
      "5/5 [==============================] - 0s 93ms/sample - loss: 2.6916e-06\n",
      "Epoch 236/300\n",
      "5/5 [==============================] - 0s 96ms/sample - loss: 2.6306e-06\n",
      "Epoch 237/300\n",
      "5/5 [==============================] - 0s 94ms/sample - loss: 2.5819e-06\n",
      "Epoch 238/300\n",
      "5/5 [==============================] - 0s 92ms/sample - loss: 2.5435e-06\n",
      "Epoch 239/300\n",
      "5/5 [==============================] - 0s 90ms/sample - loss: 2.5144e-06\n",
      "Epoch 240/300\n",
      "5/5 [==============================] - 0s 89ms/sample - loss: 2.4974e-06\n",
      "Epoch 241/300\n",
      "5/5 [==============================] - 0s 84ms/sample - loss: 2.5078e-06\n",
      "Epoch 242/300\n",
      "5/5 [==============================] - 0s 86ms/sample - loss: 2.5505e-06\n",
      "Epoch 243/300\n",
      "5/5 [==============================] - 0s 87ms/sample - loss: 2.6441e-06\n",
      "Epoch 244/300\n",
      "5/5 [==============================] - 0s 85ms/sample - loss: 2.8261e-06\n",
      "Epoch 245/300\n",
      "5/5 [==============================] - 0s 83ms/sample - loss: 3.1333e-06\n",
      "Epoch 246/300\n",
      "5/5 [==============================] - 0s 83ms/sample - loss: 3.6944e-06\n",
      "Epoch 247/300\n",
      "5/5 [==============================] - 0s 84ms/sample - loss: 4.5763e-06\n",
      "Epoch 248/300\n",
      "5/5 [==============================] - 0s 87ms/sample - loss: 6.0968e-06\n",
      "Epoch 249/300\n",
      "5/5 [==============================] - 0s 88ms/sample - loss: 8.2655e-06\n",
      "Epoch 250/300\n",
      "5/5 [==============================] - 0s 89ms/sample - loss: 1.1916e-05\n",
      "Epoch 251/300\n",
      "5/5 [==============================] - 0s 91ms/sample - loss: 1.6233e-05\n",
      "Epoch 252/300\n",
      "5/5 [==============================] - 0s 89ms/sample - loss: 2.2406e-05\n",
      "Epoch 253/300\n",
      "5/5 [==============================] - 0s 88ms/sample - loss: 2.5486e-05\n",
      "Epoch 254/300\n",
      "5/5 [==============================] - 0s 90ms/sample - loss: 2.7040e-05\n",
      "Epoch 255/300\n",
      "5/5 [==============================] - 0s 89ms/sample - loss: 2.0221e-05\n",
      "Epoch 256/300\n",
      "5/5 [==============================] - 0s 89ms/sample - loss: 1.1496e-05\n",
      "Epoch 257/300\n",
      "5/5 [==============================] - 0s 88ms/sample - loss: 3.6046e-06\n",
      "Epoch 258/300\n",
      "5/5 [==============================] - 0s 91ms/sample - loss: 1.6441e-06\n",
      "Epoch 259/300\n",
      "5/5 [==============================] - 0s 92ms/sample - loss: 5.1547e-06\n",
      "Epoch 260/300\n",
      "5/5 [==============================] - 0s 90ms/sample - loss: 1.0053e-05\n",
      "Epoch 261/300\n",
      "5/5 [==============================] - 0s 89ms/sample - loss: 1.2845e-05\n",
      "Epoch 262/300\n",
      "5/5 [==============================] - 0s 87ms/sample - loss: 1.0427e-05\n",
      "Epoch 263/300\n",
      "5/5 [==============================] - 0s 87ms/sample - loss: 5.9069e-06\n",
      "Epoch 264/300\n",
      "5/5 [==============================] - 0s 86ms/sample - loss: 2.0560e-06\n",
      "Epoch 265/300\n",
      "5/5 [==============================] - 0s 88ms/sample - loss: 1.5286e-06\n",
      "Epoch 266/300\n",
      "5/5 [==============================] - 0s 89ms/sample - loss: 3.7576e-06\n",
      "Epoch 267/300\n",
      "5/5 [==============================] - 0s 87ms/sample - loss: 6.2006e-06\n",
      "Epoch 268/300\n",
      "5/5 [==============================] - 0s 88ms/sample - loss: 7.0362e-06\n",
      "Epoch 269/300\n",
      "5/5 [==============================] - 0s 92ms/sample - loss: 5.3996e-06\n",
      "Epoch 270/300\n",
      "5/5 [==============================] - 0s 85ms/sample - loss: 3.0027e-06\n",
      "Epoch 271/300\n",
      "5/5 [==============================] - 0s 87ms/sample - loss: 1.3411e-06\n",
      "Epoch 272/300\n",
      "5/5 [==============================] - 0s 93ms/sample - loss: 1.2923e-06\n",
      "Epoch 273/300\n",
      "5/5 [==============================] - 0s 91ms/sample - loss: 2.4371e-06\n",
      "Epoch 274/300\n",
      "5/5 [==============================] - 0s 87ms/sample - loss: 3.6266e-06\n",
      "Epoch 275/300\n",
      "5/5 [==============================] - 0s 89ms/sample - loss: 4.0901e-06\n",
      "Epoch 276/300\n",
      "5/5 [==============================] - 0s 91ms/sample - loss: 3.4646e-06\n",
      "Epoch 277/300\n",
      "5/5 [==============================] - 0s 87ms/sample - loss: 2.3440e-06\n",
      "Epoch 278/300\n",
      "5/5 [==============================] - 0s 91ms/sample - loss: 1.3203e-06\n",
      "Epoch 279/300\n",
      "5/5 [==============================] - 0s 90ms/sample - loss: 9.3158e-07\n",
      "Epoch 280/300\n",
      "5/5 [==============================] - 0s 86ms/sample - loss: 1.1975e-06\n",
      "Epoch 281/300\n",
      "5/5 [==============================] - 0s 88ms/sample - loss: 1.7842e-06\n",
      "Epoch 282/300\n",
      "5/5 [==============================] - 0s 91ms/sample - loss: 2.3023e-06\n",
      "Epoch 283/300\n",
      "5/5 [==============================] - 0s 92ms/sample - loss: 2.4385e-06\n",
      "Epoch 284/300\n",
      "5/5 [==============================] - 0s 87ms/sample - loss: 2.2200e-06\n",
      "Epoch 285/300\n",
      "5/5 [==============================] - 0s 87ms/sample - loss: 1.7247e-06\n",
      "Epoch 286/300\n",
      "5/5 [==============================] - 0s 88ms/sample - loss: 1.2164e-06\n",
      "Epoch 287/300\n",
      "5/5 [==============================] - 0s 85ms/sample - loss: 8.5316e-07\n",
      "Epoch 288/300\n",
      "5/5 [==============================] - 0s 85ms/sample - loss: 7.2443e-07\n",
      "Epoch 289/300\n",
      "5/5 [==============================] - 0s 89ms/sample - loss: 8.0593e-07\n",
      "Epoch 290/300\n",
      "5/5 [==============================] - 0s 85ms/sample - loss: 1.0061e-06\n",
      "Epoch 291/300\n",
      "5/5 [==============================] - 0s 91ms/sample - loss: 1.2279e-06\n",
      "Epoch 292/300\n",
      "5/5 [==============================] - 0s 88ms/sample - loss: 1.3846e-06\n",
      "Epoch 293/300\n",
      "5/5 [==============================] - 0s 88ms/sample - loss: 1.4614e-06\n",
      "Epoch 294/300\n",
      "5/5 [==============================] - 0s 90ms/sample - loss: 1.4319e-06\n",
      "Epoch 295/300\n",
      "5/5 [==============================] - 0s 88ms/sample - loss: 1.3463e-06\n",
      "Epoch 296/300\n",
      "5/5 [==============================] - 0s 85ms/sample - loss: 1.2025e-06\n",
      "Epoch 297/300\n",
      "5/5 [==============================] - 0s 86ms/sample - loss: 1.0557e-06\n",
      "Epoch 298/300\n",
      "5/5 [==============================] - 0s 85ms/sample - loss: 9.0299e-07\n",
      "Epoch 299/300\n",
      "5/5 [==============================] - 0s 87ms/sample - loss: 7.6372e-07\n",
      "Epoch 300/300\n",
      "5/5 [==============================] - 0s 84ms/sample - loss: 6.4854e-07\n",
      "---Predicted---\n",
      "[[[0.3   0.027]\n",
      "  [0.4   0.063]\n",
      "  [0.5   0.125]]\n",
      "\n",
      " [[0.399 0.065]\n",
      "  [0.5   0.124]\n",
      "  [0.6   0.216]]\n",
      "\n",
      " [[0.5   0.125]\n",
      "  [0.6   0.216]\n",
      "  [0.699 0.343]]\n",
      "\n",
      " [[0.597 0.215]\n",
      "  [0.701 0.343]\n",
      "  [0.8   0.511]]\n",
      "\n",
      " [[0.701 0.343]\n",
      "  [0.799 0.512]\n",
      "  [0.9   0.728]]]\n",
      "---Actual---\n",
      "[[[0.3   0.027]\n",
      "  [0.4   0.064]\n",
      "  [0.5   0.125]]\n",
      "\n",
      " [[0.4   0.064]\n",
      "  [0.5   0.125]\n",
      "  [0.6   0.216]]\n",
      "\n",
      " [[0.5   0.125]\n",
      "  [0.6   0.216]\n",
      "  [0.7   0.343]]\n",
      "\n",
      " [[0.6   0.216]\n",
      "  [0.7   0.343]\n",
      "  [0.8   0.512]]\n",
      "\n",
      " [[0.7   0.343]\n",
      "  [0.8   0.512]\n",
      "  [0.9   0.729]]]\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(X, X, epochs=300, batch_size=5, verbose=1)\n",
    "# demonstrate reconstruction\n",
    "yhat = model.predict(X, verbose=0)\n",
    "print('---Predicted---')\n",
    "print(np.round(yhat,3))\n",
    "print('---Actual---')\n",
    "print(np.round(X, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#requires Tensorflow 2.0 and Tf-hub 0.7!!\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "0.7.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(hub.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/shantanu/opt/anaconda3/envs/LEXACQ/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/shantanu/opt/anaconda3/envs/LEXACQ/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "module_url = \"https://tfhub.dev/google/elmo/3\"\n",
    "embed = hub.load(module_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=tf.convert_to_tensor([\"I am feeling kind of blue\", \"Blue is the color of the sky\",\"I am feeling kind of blue\"])\n",
    "out = embed.signatures['default'](text)['elmo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pd.read_csv('final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>RT @NancyLeeGrahn: How did everyone feel about...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Positive</td>\n",
       "      <td>RT @ScottWalker: Didn't catch the full #GOPdeb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>RT @TJMShow: No mention of Tamir Rice and the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Positive</td>\n",
       "      <td>RT @RobGeorge: That Carly Fiorina is trending ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Positive</td>\n",
       "      <td>RT @DanScavino: #GOPDebate w/ @realDonaldTrump...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 sentiment                                               text\n",
       "0           0   Neutral  RT @NancyLeeGrahn: How did everyone feel about...\n",
       "1           1  Positive  RT @ScottWalker: Didn't catch the full #GOPdeb...\n",
       "2           2   Neutral  RT @TJMShow: No mention of Tamir Rice and the ...\n",
       "3           3  Positive  RT @RobGeorge: That Carly Fiorina is trending ...\n",
       "4           4  Positive  RT @DanScavino: #GOPDebate w/ @realDonaldTrump..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = text[text.sentiment == 'Negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.iloc[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text Cleaning \n",
    "\n",
    "# tweets.text = tweets.text.str.lower()\n",
    "\n",
    "tweets.text = tweets.text.str.replace(\"&amp\",\"\")\n",
    "tweets.text = tweets.text.str.replace(\"\\n\",\" \")\n",
    "tweets.text = tweets.text.str.replace(\"\\\\\\\\\",\" \")\n",
    "\n",
    "### Deleting Multiple Spaces \n",
    "\n",
    "tweets.text =  [re.sub(' +', ' ', str(x)) for x in tweets['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=tf.convert_to_tensor([str(list(tweets.text.values))])\n",
    "out = embed.signatures['default'](text)['elmo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 7, 1024])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 7, 1024])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([[-0.73210824, -0.3453857 , -0.124824  , ..., -0.18160583,\n",
       "         0.4167027 ,  0.24779648],\n",
       "       [-0.21743327, -0.1413041 , -0.01641104, ...,  0.09404293,\n",
       "         0.06934943, -0.25008377],\n",
       "       [-0.15964414, -0.01390902, -0.16066319, ...,  0.564662  ,\n",
       "         0.42623278,  0.57753956]], dtype=float32),\n",
       "       array([[-0.46022922, -0.4642863 ,  0.14044693, ..., -0.02865595,\n",
       "        -0.14250925, -1.1547729 ],\n",
       "       [-0.20208055,  0.06410888,  0.01537442, ...,  0.42384213,\n",
       "         0.15774754, -0.6359169 ],\n",
       "       [-0.56034267,  0.01419745,  0.37376517, ...,  0.60058975,\n",
       "         0.11053625, -0.12548313]], dtype=float32),\n",
       "       array([], shape=(0, 1024), dtype=float32)], dtype=object)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 151, 100)          20300     \n",
      "_________________________________________________________________\n",
      "simple_rnn_6 (SimpleRNN)     (None, 151, 128)          29312     \n",
      "_________________________________________________________________\n",
      "simple_rnn_7 (SimpleRNN)     (None, 40)                6760      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 82        \n",
      "=================================================================\n",
      "Total params: 56,454\n",
      "Trainable params: 56,454\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "\n",
    "# define model\n",
    "model_TAE = Sequential()\n",
    "model_TAE .add(LSTM(1024, activation='relu', input_shape=(timesteps,n_features), return_sequences=True))\n",
    "model_TAE .add(LSTM(720, activation='relu', input_shape=(timesteps,n_features), return_sequences=True))\n",
    "model_TAE .add(LSTM(128, activation='relu', input_shape=(timesteps,n_features), return_sequences=True))\n",
    "model_TAE .add(LSTM(64, activation='relu', return_sequences=False))\n",
    "model_TAE .add(RepeatVector(timesteps))\n",
    "model_TAE .add(LSTM(64, activation='relu', return_sequences=True))\n",
    "model_TAE .add(LSTM(128, activation='relu', return_sequences=True))\n",
    "model_TAE .add(LSTM(720, activation='relu', return_sequences=True))\n",
    "model_TAE .add(LSTM(1027, activation='relu', return_sequences=True))\n",
    "model_TAE .add(TimeDistributed(Dense(n_features)))\n",
    "model_TAE .compile(optimizer='adam', loss='mse')\n",
    "model.summary()\n",
    "# model_TAE .fit(aa, aa, epochs=3, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 3 into shape (3,3,1024)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-254-e83478caed31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 3 into shape (3,3,1024)"
     ]
    }
   ],
   "source": [
    "aa.reshape(aa.shape[0],3,1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3 samples\n",
      "Epoch 1/3\n",
      "3/3 [==============================] - 15s 5s/sample - loss: nan\n",
      "Epoch 2/3\n",
      "3/3 [==============================] - 0s 146ms/sample - loss: nan\n",
      "Epoch 3/3\n",
      "3/3 [==============================] - 0s 155ms/sample - loss: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1af6a2a210>"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_TAE .fit(aa, aa, epochs=3, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Predicted---\n",
      "[[[-1.235 -0.566 -0.178 ... -0.25   0.243  0.375]\n",
      "  [-0.464 -0.697 -0.437 ...  0.539  0.952  0.016]\n",
      "  [-0.288  0.496  0.338 ...  0.291  0.708  0.739]\n",
      "  ...\n",
      "  [ 0.163 -0.189  0.24  ... -1.57   0.661  0.116]\n",
      "  [-0.585 -0.742  0.053 ...  0.24   0.311  0.319]\n",
      "  [-0.029 -0.044  0.04  ...  0.027 -0.013 -0.017]]\n",
      "\n",
      " [[-0.634 -0.312 -0.31  ...  0.194 -0.653  0.008]\n",
      "  [-0.117  0.029  0.314 ... -0.376  0.763  0.332]\n",
      "  [ 0.144 -0.207  0.477 ...  0.135  0.041 -0.109]\n",
      "  ...\n",
      "  [-0.183  0.184 -0.156 ... -1.519  0.263 -0.357]\n",
      "  [ 0.531  0.     0.062 ...  0.019  0.306 -0.491]\n",
      "  [ 0.319  0.254 -0.507 ...  0.168  0.236  0.735]]\n",
      "\n",
      " [[-1.235 -0.566 -0.178 ... -0.25   0.243  0.375]\n",
      "  [-0.464 -0.697 -0.437 ...  0.539  0.952  0.016]\n",
      "  [-0.288  0.496  0.338 ...  0.291  0.708  0.739]\n",
      "  ...\n",
      "  [ 0.163 -0.189  0.24  ... -1.57   0.661  0.116]\n",
      "  [-0.585 -0.742  0.053 ...  0.24   0.311  0.319]\n",
      "  [-0.029 -0.044  0.04  ...  0.027 -0.013 -0.017]]]\n",
      "---Actual---\n",
      "[[[-1.225 -0.56  -0.176 ... -0.25   0.242  0.374]\n",
      "  [-0.463 -0.688 -0.433 ...  0.529  0.943  0.018]\n",
      "  [-0.286  0.49   0.333 ...  0.286  0.704  0.73 ]\n",
      "  ...\n",
      "  [ 0.164 -0.184  0.242 ... -1.551  0.65   0.113]\n",
      "  [-0.578 -0.727  0.056 ...  0.227  0.304  0.31 ]\n",
      "  [-0.028 -0.044  0.041 ...  0.026 -0.014 -0.017]]\n",
      "\n",
      " [[-0.631 -0.31  -0.308 ...  0.191 -0.65   0.008]\n",
      "  [-0.116  0.03   0.313 ... -0.375  0.757  0.329]\n",
      "  [ 0.144 -0.204  0.476 ...  0.135  0.041 -0.107]\n",
      "  ...\n",
      "  [-0.178  0.187 -0.151 ... -1.515  0.263 -0.358]\n",
      "  [ 0.527  0.006  0.062 ...  0.007  0.308 -0.488]\n",
      "  [ 0.314  0.253 -0.5   ...  0.152  0.236  0.721]]\n",
      "\n",
      " [[-1.225 -0.56  -0.176 ... -0.25   0.242  0.374]\n",
      "  [-0.463 -0.688 -0.433 ...  0.529  0.943  0.018]\n",
      "  [-0.286  0.49   0.333 ...  0.286  0.704  0.73 ]\n",
      "  ...\n",
      "  [ 0.164 -0.184  0.242 ... -1.551  0.65   0.113]\n",
      "  [-0.578 -0.727  0.056 ...  0.227  0.304  0.31 ]\n",
      "  [-0.028 -0.044  0.041 ...  0.026 -0.014 -0.017]]]\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "# model.fit(X, X, epochs=300, batch_size=5, verbose=0)\n",
    "# # demonstrate reconstruction\n",
    "yhat = model.predict(X, verbose=0)\n",
    "print('---Predicted---')\n",
    "print(np.round(yhat,3))\n",
    "print('---Actual---')\n",
    "print(np.round(X, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=44796, shape=(), dtype=float32, numpy=0.029630631>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.11707014,  0.02946293,  0.31371886, ..., -0.3759995 ,\n",
       "        0.76296276,  0.33222032], dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9999984 , 0.22021209, 0.17488632, 0.19086945, 0.12718728,\n",
       "        0.18547067, 0.13892356],\n",
       "       [0.22004215, 0.99999267, 0.43366843, 0.27878729, 0.31209975,\n",
       "        0.21988192, 0.2538378 ],\n",
       "       [0.17521492, 0.43285212, 0.9999906 , 0.41001785, 0.28900406,\n",
       "        0.57374215, 0.19866472],\n",
       "       [0.1898911 , 0.27963036, 0.41042745, 0.99998736, 0.28019905,\n",
       "        0.27675462, 0.37575626],\n",
       "       [0.12598501, 0.31087074, 0.2900501 , 0.27850658, 0.9999824 ,\n",
       "        0.4377004 , 0.26187444],\n",
       "       [0.18399483, 0.22046381, 0.57457495, 0.2792754 , 0.44138718,\n",
       "        0.9999609 , 0.3052461 ],\n",
       "       [0.1402052 , 0.25613952, 0.20154655, 0.37691993, 0.27127737,\n",
       "        0.31106085, 0.9999559 ]], dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(X[1],yhat[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pd.read_csv('final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>RT @NancyLeeGrahn: How did everyone feel about...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Positive</td>\n",
       "      <td>RT @ScottWalker: Didn't catch the full #GOPdeb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>RT @TJMShow: No mention of Tamir Rice and the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Positive</td>\n",
       "      <td>RT @RobGeorge: That Carly Fiorina is trending ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Positive</td>\n",
       "      <td>RT @DanScavino: #GOPDebate w/ @realDonaldTrump...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 sentiment                                               text\n",
       "0           0   Neutral  RT @NancyLeeGrahn: How did everyone feel about...\n",
       "1           1  Positive  RT @ScottWalker: Didn't catch the full #GOPdeb...\n",
       "2           2   Neutral  RT @TJMShow: No mention of Tamir Rice and the ...\n",
       "3           3  Positive  RT @RobGeorge: That Carly Fiorina is trending ...\n",
       "4           4  Positive  RT @DanScavino: #GOPDebate w/ @realDonaldTrump..."
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Data.text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a.translate(str.maketrans('','',string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT', 'ScottWalker', 'Didnt']\n",
      "['ScottWalker', 'Didnt', 'catch']\n",
      "['Didnt', 'catch', 'full']\n",
      "['catch', 'full', 'GOPdebate']\n",
      "['full', 'GOPdebate', 'last']\n",
      "['GOPdebate', 'last', 'night']\n",
      "['last', 'night', 'Here']\n",
      "['night', 'Here', 'Scotts']\n",
      "['Here', 'Scotts', 'best']\n",
      "['Scotts', 'best', 'lines']\n",
      "['best', 'lines', '90']\n",
      "['lines', '90', 'seconds']\n",
      "['90', 'seconds', 'Walker16']\n",
      "['seconds', 'Walker16', 'httptcoZSfF…']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(a)\n",
    "from nltk.corpus import stopwords \n",
    "tokens_without_sw = [word for word in tokens if not word in stopwords.words()]\n",
    "for i in range(len(tokens_without_sw)-2):\n",
    "    print(tokens_without_sw[i:i+3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens_without_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT', 'ScottWalker', 'Didnt']\n",
      "['ScottWalker', 'Didnt', 'catch']\n",
      "['Didnt', 'catch', 'full']\n",
      "['catch', 'full', 'GOPdebate']\n",
      "['full', 'GOPdebate', 'last']\n",
      "['GOPdebate', 'last', 'night']\n",
      "['last', 'night', 'Here']\n",
      "['night', 'Here', 'Scotts']\n",
      "['Here', 'Scotts', 'best']\n",
      "['Scotts', 'best', 'lines']\n",
      "['best', 'lines', '90']\n",
      "['lines', '90', 'seconds']\n",
      "['90', 'seconds', 'Walker16']\n",
      "['seconds', 'Walker16', 'httptcoZSfF…']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(tokens_without_sw)-2):\n",
    "    print(tokens_without_sw[i:i+3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "a = K.placeholder(shape=(None, 2,2))\n",
    "b = K.ones_like(a)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.Tensor"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 2, 2])"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(arr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 2)"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.concat([a,arr1],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"strided_slice_3:0\", shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(a[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot use the given session to evaluate tensor: the tensor's graph is different from the session's graph.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-345-7a8850810e7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/LEXACQ/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m     \"\"\"\n\u001b[0;32m--> 798\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mexperimental_ref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/LEXACQ/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   5405\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5406\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5407\u001b[0;31m       raise ValueError(\"Cannot use the given session to evaluate tensor: \"\n\u001b[0m\u001b[1;32m   5408\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5409\u001b[0m                        \"graph.\")\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot use the given session to evaluate tensor: the tensor's graph is different from the session's graph."
     ]
    }
   ],
   "source": [
    "with tf.compat.v1.Session() as sess:  print(a.eval(session=sess)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
