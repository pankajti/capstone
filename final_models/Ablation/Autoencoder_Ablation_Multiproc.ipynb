{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "max_words = 15000\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "maxlen = 150\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/shantanu/opt/anaconda3/envs/LEXACQ/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "module_url = \"https://tfhub.dev/google/elmo/3\"\n",
    "embed = hub.load(\"/Users/shantanu/elmo_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (115864, 150)\n",
      "Shape of label tensor: (115864, 2)\n"
     ]
    }
   ],
   "source": [
    "root_dir = r'/Users/shantanu/Library/Mobile Documents/com~apple~CloudDocs/Capstone/Wikipedia Data'\n",
    "\n",
    "result_path = annot_file_path = os.path.join(root_dir , 'comments_with_grouped_annoptations.tsv')\n",
    "\n",
    "merged_comments = pd.read_table(result_path)\n",
    "merged_comments['recipient_attack'] = merged_comments['recipient_attack'].apply(lambda x : 1 if x> 1 else 0 )\n",
    "X_train = merged_comments['comment']\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences = tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "y = merged_comments['recipient_attack']\n",
    "\n",
    "one_hot_train_labels = to_categorical(y)\n",
    "\n",
    "labels = np.asarray(one_hot_train_labels)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "labels = labels[indices]\n",
    "\n",
    "\n",
    "X_train, X_test_validate, y_train, y_test_validate = train_test_split(merged_comments.comment, merged_comments.recipient_attack,\n",
    "                                                    stratify=labels,\n",
    "                                                    test_size=0.995,\n",
    "                                                    random_state=123)\n",
    "\n",
    "DF_train = X_train.to_frame()\n",
    "DF_train['recipient_attack'] = y_train\n",
    "\n",
    "DF_train.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"../tokenizer.pkl\",\"rb\") as f:\n",
    "    tokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model load \n",
    "\n",
    "model = load_model('../basic_lstm_with_auto_encoder.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "\n",
    "def preprocess_text(text_eval):\n",
    "    sequences = tokenizer.texts_to_sequences([text_eval])\n",
    "    data = pad_sequences(sequences, maxlen=maxlen)\n",
    "    return data\n",
    "\n",
    "\n",
    "def abalation(args  ):\n",
    "    \n",
    "    pred_vals_array = np.empty((0,2), float)\n",
    "    NAE_vals_array = np.empty((3,1024), float)\n",
    "    PAE_vals_array = np.empty((3,1024), float)\n",
    "\n",
    "    from tensorflow.keras.models import load_model as lm1\n",
    "    index, row = args\n",
    "    print(\"running for {}\".format(index))\n",
    "\n",
    "\n",
    "    text = row.comment.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens_without_sw = tokens #[word for word in tokens if not word in stopwords.words()]\n",
    "    \n",
    "\n",
    "    ### First Get the overall Classification\n",
    "\n",
    "#     print(tokens_without_sw)\n",
    "    \n",
    "    tokens_string = str(tokens_without_sw)\n",
    "    \n",
    "    prep_text = preprocess_text(tokens_string)\n",
    "    \n",
    "    pred_vlaues = model.predict(prep_text)\n",
    "\n",
    "    classification = pred_vlaues.argmax()\n",
    "    \n",
    "    if classification == 0 and row.recipient_attack == 0:\n",
    "        mode = 'PAE'  # Positive AutoEncoder\n",
    "    elif classification == 0 and row.recipient_attack == 1:\n",
    "        mode = 'NAE'\n",
    "    elif classification == 1 and row.recipient_attack == 0:\n",
    "        mode = 'PAE'\n",
    "    elif classification == 1 and row.recipient_attack == 1:\n",
    "        mode = 'NAE'\n",
    "\n",
    "    ### Get the Elmo vector\n",
    "    \n",
    "    text = tf.convert_to_tensor([str(tokens_without_sw)])\n",
    "    out = embed.signatures['default'](text)['elmo']\n",
    "\n",
    "    pred_vals_array = np.empty((0, 2), float)\n",
    "    \n",
    "    for i in range(len(tokens_without_sw) - 2):\n",
    "        \n",
    "        text_eval = tokens_without_sw[i:i + 3]\n",
    "        \n",
    "        \n",
    "        a = preprocess_text(text_eval)\n",
    "        \n",
    "    \n",
    "        pred_vlaues = model.predict(a)\n",
    "    \n",
    "          \n",
    "        pred_vals_array = np.vstack((pred_vals_array, pred_vlaues))\n",
    "        \n",
    "        \n",
    "    \n",
    "    if mode == \"NAE\":\n",
    "\n",
    "        # Only Negative Vals, and find the index of the highest contributing phrase\n",
    "\n",
    "        phrase_start_index = pred_vals_array[:, 1].argmax()\n",
    "\n",
    "        ### Get the ELMO Encoding of the entire vector:\n",
    "\n",
    "        elmo_vecs_NAE = np.array(out[0][phrase_start_index:phrase_start_index + 3])\n",
    "\n",
    "        \n",
    "        NAE_vals_array = np.vstack((NAE_vals_array, elmo_vecs_NAE))\n",
    "        \n",
    "#         final_list_NAE.append(elmo_vecs_NAE)\n",
    "\n",
    "    elif mode == \"PAE\":\n",
    "\n",
    "        # Only Positive Vals, and find the index of the highest contributing phrase\n",
    "\n",
    "        phrase_start_index = pred_vals_array[:, 0].argmax()\n",
    "        \n",
    "        elmo_vecs_PAE = out[0][phrase_start_index:phrase_start_index + 3]\n",
    "\n",
    "        PAE_vals_array = np.vstack((PAE_vals_array, elmo_vecs_PAE))\n",
    "        \n",
    "#         final_list_PAE.append(elmo_vecs_PAE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>comment</th>\n",
       "      <th>recipient_attack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>6443</td>\n",
       "      <td>CategoryNEWLINE_TOKENNEWLINE_TOKENIt has been ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>97387</td>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKENThat is fucking ridi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>64575</td>\n",
       "      <td>Extended comments about thisNEWLINE_TOKENThe c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>37597</td>\n",
       "      <td>`NEWLINE_TOKENNEWLINE_TOKENYou're showing sign...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>10037</td>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKENLatest press reports...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                                            comment  \\\n",
       "574   6443  CategoryNEWLINE_TOKENNEWLINE_TOKENIt has been ...   \n",
       "575  97387  NEWLINE_TOKENNEWLINE_TOKENThat is fucking ridi...   \n",
       "576  64575  Extended comments about thisNEWLINE_TOKENThe c...   \n",
       "577  37597  `NEWLINE_TOKENNEWLINE_TOKENYou're showing sign...   \n",
       "578  10037  NEWLINE_TOKENNEWLINE_TOKENLatest press reports...   \n",
       "\n",
       "     recipient_attack  \n",
       "574                 0  \n",
       "575                 1  \n",
       "576                 0  \n",
       "577                 0  \n",
       "578                 0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.DatasetV2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.data.Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.data.Dataset()\n",
    "# .map(\n",
    "#     a,\n",
    "#     num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sequential ():\n",
    "    for rec in DF_train.iterrows():\n",
    "        abalation(rec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:LEXACQ]",
   "language": "python",
   "name": "conda-env-LEXACQ-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
